
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Introduction to Artificial Neural Networks with Keras &#8212; From Statistics to Statistical Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://integzz.github.io/excss-stats/handson-dl/101-intro-keras.html" />
    <link rel="shortcut icon" href="../_static/logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Training Deep Neural Networks" href="102-training-deep-neural-networks.html" />
    <link rel="prev" title="Ensemble Learning" href="../handson/07-ensemble-learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">From Statistics to Statistical Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   From Statistics to Statistical Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  HandsON
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../handson/02-data-processing.html">
   Data Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../handson/03-classification.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../handson/04-linear-models.html">
   Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../handson/05-support-vector-machines.html">
   Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../handson/06-decision-trees.html">
   Decision Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../handson/07-ensemble-learning.html">
   Ensemble Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  HandsON DL
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction to Artificial Neural Networks with Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="102-training-deep-neural-networks.html">
   Training Deep Neural Networks
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/handson-dl/101-intro-keras.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/integzz/excss-stats/"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/integzz/excss-stats//issues/new?title=Issue%20on%20page%20%2Fhandson-dl/101-intro-keras.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/integzz/excss-stats/edit/master/handson-dl/101-intro-keras.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/integzz/excss-stats/master?urlpath=lab/tree/handson-dl/101-intro-keras.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/integzz/excss-stats/blob/master/handson-dl/101-intro-keras.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Introduction to Artificial Neural Networks with Keras
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#perceptrons">
   Perceptrons
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions">
   Activation functions
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-an-image-classifier">
   Building an Image Classifier
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regression-mlp">
   Regression MLP
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#functional-api">
   Functional API
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-subclassing-api">
   The subclassing API
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#saving-and-restoring">
   Saving and Restoring
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-callbacks-during-training">
   Using Callbacks during Training
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensorboard">
   TensorBoard
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyperparameter-tuning">
   Hyperparameter Tuning
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-solutions">
   Exercise solutions
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="introduction-to-artificial-neural-networks-with-keras">
<h1>Introduction to Artificial Neural Networks with Keras<a class="headerlink" href="#introduction-to-artificial-neural-networks-with-keras" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="perceptrons">
<h1>Perceptrons<a class="headerlink" href="#perceptrons" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Perceptron</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;sepal length (cm)&#39;,
 &#39;sepal width (cm)&#39;,
 &#39;petal length (cm)&#39;,
 &#39;petal width (cm)&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]</span>  <span class="c1"># petal length, petal width</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">per_clf</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">per_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">per_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>
<span class="n">y_pred</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">per_clf</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-1.4, -2.2]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">per_clf</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([4.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="o">-</span><span class="n">per_clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">per_clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="n">per_clf</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">/</span> <span class="n">per_clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

<span class="n">lims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="n">x0</span><span class="p">,</span> <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">lims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">lims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">500</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">lims</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">lims</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="mi">200</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">)</span>
    
<span class="n">x0</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(200, 500)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">x1</span><span class="o">.</span><span class="n">flatten</span><span class="p">()]</span>
<span class="n">X_new</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.        , 0.        ],
       [0.01002004, 0.        ],
       [0.02004008, 0.        ],
       ...,
       [4.97995992, 2.        ],
       [4.98997996, 2.        ],
       [5.        , 2.        ]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_predict</span> <span class="o">=</span> <span class="n">per_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
<span class="n">y_predict</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1, 1, 1, ..., 0, 0, 0])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zz</span> <span class="o">=</span> <span class="n">y_predict</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">zz</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;bs&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Not Iris-Setosa&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;yo&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Iris-Setosa&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">lims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">lims</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="n">a</span> <span class="o">*</span> <span class="n">lims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">*</span> <span class="n">lims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">],</span>
        <span class="s2">&quot;k-&quot;</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">custom_cmap</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#9898ff&#39;</span><span class="p">,</span> <span class="s1">&#39;#fafab0&#39;</span><span class="p">])</span>

<span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">zz</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">custom_cmap</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Petal length&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Petal width&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">lims</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/101-intro-keras_13_0.png" src="../_images/101-intro-keras_13_0.png" />
</div>
</div>
</div>
<div class="section" id="activation-functions">
<h1>Activation functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">z</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">f</span><span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">eps</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>
                       <span class="mi">2</span><span class="p">,</span>
                       <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
                       <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                       <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">funcs</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">relu</span><span class="p">]</span>
<span class="n">styles</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;r-&quot;</span><span class="p">,</span> <span class="s2">&quot;g--&quot;</span><span class="p">,</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span> <span class="s2">&quot;m-.&quot;</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Step&quot;</span><span class="p">,</span> <span class="s2">&quot;Sigmoid&quot;</span><span class="p">,</span> <span class="s2">&quot;Tanh&quot;</span><span class="p">,</span> <span class="s2">&quot;ReLU&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">func</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">funcs</span><span class="p">,</span> <span class="n">styles</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">style</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">derivative</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span> <span class="n">style</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>

<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Activation functions&quot;</span><span class="p">,</span> <span class="s2">&quot;Derivatives of activation functions&quot;</span><span class="p">]</span>
<span class="n">limss</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mf">.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">]]</span>

<span class="k">for</span> <span class="n">title</span><span class="p">,</span> <span class="n">lims</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">titles</span><span class="p">,</span> <span class="n">limss</span><span class="p">,</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">lims</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;center right&quot;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;ro&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;rx&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/101-intro-keras_16_0.png" src="../_images/101-intro-keras_16_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">heaviside</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">z</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">mlp_xor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">heaviside</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">activation</span><span class="p">(</span><span class="o">-</span><span class="n">activation</span><span class="p">(</span><span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">-</span> <span class="mf">1.5</span><span class="p">)</span> <span class="o">+</span> <span class="n">activation</span><span class="p">(</span><span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-</span>
                      <span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x1s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x2s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="n">x2s</span><span class="p">)</span>

<span class="n">z1</span> <span class="o">=</span> <span class="n">mlp_xor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">heaviside</span><span class="p">)</span>
<span class="n">z2</span> <span class="o">=</span> <span class="n">mlp_xor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">sigmoid</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">z</span> <span class="o">=</span> <span class="p">[</span><span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="p">]</span>
<span class="n">funcs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;heaviside&#39;</span><span class="p">,</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">z</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;gs&quot;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;y^&quot;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Activation function: </span><span class="si">{</span><span class="n">funcs</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/101-intro-keras_18_0.png" src="../_images/101-intro-keras_18_0.png" />
</div>
</div>
</div>
<div class="section" id="building-an-image-classifier">
<h1>Building an Image Classifier<a class="headerlink" href="#building-an-image-classifier" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fashion_mnist</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">fashion_mnist</span>
<span class="p">(</span><span class="n">X_train_full</span><span class="p">,</span> <span class="n">y_train_full</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">fashion_mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_full</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(60000, 28, 28)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_full</span><span class="o">.</span><span class="n">dtype</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dtype(&#39;uint8&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_valid</span><span class="p">,</span> <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train_full</span><span class="p">[:</span><span class="mi">5000</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.</span><span class="p">,</span> <span class="n">X_train_full</span><span class="p">[</span><span class="mi">5000</span><span class="p">:]</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="n">y_valid</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train_full</span><span class="p">[:</span><span class="mi">5000</span><span class="p">],</span> <span class="n">y_train_full</span><span class="p">[</span><span class="mi">5000</span><span class="p">:]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span> <span class="o">/</span> <span class="mf">255.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/101-intro-keras_25_0.png" src="../_images/101-intro-keras_25_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([4, 0, 7, ..., 3, 0, 5], dtype=uint8)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;T-shirt/top&quot;</span><span class="p">,</span> <span class="s2">&quot;Trouser&quot;</span><span class="p">,</span> <span class="s2">&quot;Pullover&quot;</span><span class="p">,</span> <span class="s2">&quot;Dress&quot;</span><span class="p">,</span> <span class="s2">&quot;Coat&quot;</span><span class="p">,</span> <span class="s2">&quot;Sandal&quot;</span><span class="p">,</span> <span class="s2">&quot;Shirt&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Sneaker&quot;</span><span class="p">,</span> <span class="s2">&quot;Bag&quot;</span><span class="p">,</span> <span class="s2">&quot;Ankle boot&quot;</span>
<span class="p">]</span>
<span class="n">class_names</span><span class="p">[</span><span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Coat&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_valid</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(5000, 28, 28)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(10000, 28, 28)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span>
                       <span class="mi">10</span><span class="p">,</span>
                       <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                       <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                       <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">ind</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">y_train</span><span class="p">[</span><span class="n">ind</span><span class="p">]],</span> <span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;small&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/101-intro-keras_30_0.png" src="../_images/101-intro-keras_30_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2021-09-19 20:51:10.998916: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">layers</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;tensorflow.python.keras.layers.core.Flatten at 0x147a68e50&gt;,
 &lt;tensorflow.python.keras.layers.core.Dense at 0x14792d0a0&gt;,
 &lt;tensorflow.python.keras.layers.core.Dense at 0x147a68cd0&gt;,
 &lt;tensorflow.python.keras.layers.core.Dense at 0x147a68f70&gt;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
dense (Dense)                (None, 300)               235500    
_________________________________________________________________
dense_1 (Dense)              (None, 100)               30100     
_________________________________________________________________
dense_2 (Dense)              (None, 10)                1010      
=================================================================
Total params: 266,610
Trainable params: 266,610
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pydot</span>
<span class="kn">import</span> <span class="nn">graphviz</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;data/101/my_fashion_mnist_model.png&quot;</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&#39;Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), &#39;, &#39;for `pydotprint` to work.&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hidden1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">hidden1</span><span class="o">.</span><span class="n">name</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;dense&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="n">hidden1</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="ow">is</span> <span class="n">hidden1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="n">hidden1</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0.02448617, -0.00877795, -0.02189048, ..., -0.02766046,
         0.03859074, -0.06889391],
       [ 0.00476504, -0.03105379, -0.0586676 , ...,  0.00602964,
        -0.02763776, -0.04165364],
       [-0.06189284, -0.06901957,  0.07102345, ..., -0.04238207,
         0.07121518, -0.07331658],
       ...,
       [-0.03048757,  0.02155137, -0.05400612, ..., -0.00113463,
         0.00228987,  0.05581069],
       [ 0.07061854, -0.06960931,  0.07038955, ..., -0.00384101,
         0.00034875,  0.02878492],
       [-0.06022581,  0.01577859, -0.02585464, ..., -0.00527829,
         0.00272203, -0.06793761]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(784, 300)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">biases</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">biases</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(300,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;sparse_categorical_crossentropy&quot;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;sgd&quot;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2021-09-19 20:51:39.165510: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/30
1719/1719 [==============================] - 14s 7ms/step - loss: 1.0188 - accuracy: 0.6805 - val_loss: 0.5218 - val_accuracy: 0.8210
Epoch 2/30
1719/1719 [==============================] - 10s 6ms/step - loss: 0.5028 - accuracy: 0.8260 - val_loss: 0.4354 - val_accuracy: 0.8522
Epoch 3/30
1719/1719 [==============================] - 10s 6ms/step - loss: 0.4484 - accuracy: 0.8425 - val_loss: 0.5316 - val_accuracy: 0.7988
Epoch 4/30
1719/1719 [==============================] - 11s 6ms/step - loss: 0.4207 - accuracy: 0.8525 - val_loss: 0.3914 - val_accuracy: 0.8650
Epoch 5/30
1719/1719 [==============================] - 9s 5ms/step - loss: 0.4060 - accuracy: 0.8582 - val_loss: 0.3745 - val_accuracy: 0.8690
Epoch 6/30
1719/1719 [==============================] - 11s 7ms/step - loss: 0.3751 - accuracy: 0.8679 - val_loss: 0.3707 - val_accuracy: 0.8728
Epoch 7/30
1719/1719 [==============================] - 14s 8ms/step - loss: 0.3649 - accuracy: 0.8710 - val_loss: 0.3630 - val_accuracy: 0.8718
Epoch 8/30
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3476 - accuracy: 0.8755 - val_loss: 0.3849 - val_accuracy: 0.8620
Epoch 9/30
1719/1719 [==============================] - 9s 5ms/step - loss: 0.3482 - accuracy: 0.8756 - val_loss: 0.3597 - val_accuracy: 0.8682
Epoch 10/30
1719/1719 [==============================] - 9s 6ms/step - loss: 0.3293 - accuracy: 0.8840 - val_loss: 0.3431 - val_accuracy: 0.8780
Epoch 11/30
1719/1719 [==============================] - 10s 6ms/step - loss: 0.3212 - accuracy: 0.8839 - val_loss: 0.3443 - val_accuracy: 0.8782
Epoch 12/30
1719/1719 [==============================] - 12s 7ms/step - loss: 0.3121 - accuracy: 0.8869 - val_loss: 0.3308 - val_accuracy: 0.8832
Epoch 13/30
1719/1719 [==============================] - 13s 7ms/step - loss: 0.3050 - accuracy: 0.8899 - val_loss: 0.3274 - val_accuracy: 0.8870
Epoch 14/30
1719/1719 [==============================] - 9s 5ms/step - loss: 0.2988 - accuracy: 0.8918 - val_loss: 0.3407 - val_accuracy: 0.8774
Epoch 15/30
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2931 - accuracy: 0.8948 - val_loss: 0.3212 - val_accuracy: 0.8858
Epoch 16/30
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2859 - accuracy: 0.8981 - val_loss: 0.3093 - val_accuracy: 0.8902
Epoch 17/30
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2778 - accuracy: 0.9005 - val_loss: 0.3561 - val_accuracy: 0.8730
Epoch 18/30
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2774 - accuracy: 0.9005 - val_loss: 0.3136 - val_accuracy: 0.8910
Epoch 19/30
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2736 - accuracy: 0.9023 - val_loss: 0.3127 - val_accuracy: 0.8908
Epoch 20/30
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2692 - accuracy: 0.9045 - val_loss: 0.3281 - val_accuracy: 0.8818
Epoch 21/30
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2666 - accuracy: 0.9055 - val_loss: 0.3057 - val_accuracy: 0.8926
Epoch 22/30
1719/1719 [==============================] - 9s 5ms/step - loss: 0.2610 - accuracy: 0.9056 - val_loss: 0.2963 - val_accuracy: 0.8962
Epoch 23/30
1719/1719 [==============================] - 12s 7ms/step - loss: 0.2545 - accuracy: 0.9074 - val_loss: 0.3000 - val_accuracy: 0.8918
Epoch 24/30
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2446 - accuracy: 0.9121 - val_loss: 0.3076 - val_accuracy: 0.8884
Epoch 25/30
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2485 - accuracy: 0.9116 - val_loss: 0.2963 - val_accuracy: 0.8942
Epoch 26/30
1719/1719 [==============================] - 10s 6ms/step - loss: 0.2423 - accuracy: 0.9133 - val_loss: 0.3086 - val_accuracy: 0.8882
Epoch 27/30
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2366 - accuracy: 0.9166 - val_loss: 0.3013 - val_accuracy: 0.8956
Epoch 28/30
1719/1719 [==============================] - 11s 6ms/step - loss: 0.2309 - accuracy: 0.9168 - val_loss: 0.2995 - val_accuracy: 0.8942
Epoch 29/30
1193/1719 [===================&gt;..........] - ETA: 3s - loss: 0.2274 - accuracy: 0.9171
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="mi">93</span><span class="o">/</span><span class="mi">795</span><span class="n">zm8c93m16_92qkk86t0_r0000gn</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_20728</span><span class="o">/</span><span class="mf">383415245.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span>                     <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>

<span class="nn">/usr/local/Caskroom/miniconda/base/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py</span> in <span class="ni">fit</span><span class="nt">(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)</span>
<span class="g g-Whitespace">   </span><span class="mi">1093</span>                 <span class="n">_r</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1094</span>               <span class="n">callbacks</span><span class="o">.</span><span class="n">on_train_batch_begin</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1095</span>               <span class="n">tmp_logs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_function</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1096</span>               <span class="k">if</span> <span class="n">data_handler</span><span class="o">.</span><span class="n">should_sync</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1097</span>                 <span class="n">context</span><span class="o">.</span><span class="n">async_wait</span><span class="p">()</span>

<span class="nn">/usr/local/Caskroom/miniconda/base/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py</span> in <span class="ni">__call__</span><span class="nt">(self, *args, **kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">826</span>     <span class="n">tracing_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">experimental_get_tracing_count</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">827</span>     <span class="k">with</span> <span class="n">trace</span><span class="o">.</span><span class="n">Trace</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">)</span> <span class="k">as</span> <span class="n">tm</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">828</span>       <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">829</span>       <span class="n">compiler</span> <span class="o">=</span> <span class="s2">&quot;xla&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_experimental_compile</span> <span class="k">else</span> <span class="s2">&quot;nonXla&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">830</span>       <span class="n">new_tracing_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">experimental_get_tracing_count</span><span class="p">()</span>

<span class="nn">/usr/local/Caskroom/miniconda/base/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py</span> in <span class="ni">_call</span><span class="nt">(self, *args, **kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">853</span>       <span class="c1"># In this case we have created variables on the first call, so we run the</span>
<span class="g g-Whitespace">    </span><span class="mi">854</span>       <span class="c1"># defunned version which is guaranteed to never create variables.</span>
<span class="ne">--&gt; </span><span class="mi">855</span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>  <span class="c1"># pylint: disable=not-callable</span>
<span class="g g-Whitespace">    </span><span class="mi">856</span>     <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">857</span>       <span class="c1"># Release the lock early so that multiple threads can perform the call</span>

<span class="nn">/usr/local/Caskroom/miniconda/base/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/eager/function.py</span> in <span class="ni">__call__</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">2940</span>       <span class="p">(</span><span class="n">graph_function</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2941</span>        <span class="n">filtered_flat_args</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_define_function</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">2942</span>     <span class="k">return</span> <span class="n">graph_function</span><span class="o">.</span><span class="n">_call_flat</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2943</span>         <span class="n">filtered_flat_args</span><span class="p">,</span> <span class="n">captured_inputs</span><span class="o">=</span><span class="n">graph_function</span><span class="o">.</span><span class="n">captured_inputs</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
<span class="g g-Whitespace">   </span><span class="mi">2944</span> 

<span class="nn">/usr/local/Caskroom/miniconda/base/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/eager/function.py</span> in <span class="ni">_call_flat</span><span class="nt">(self, args, captured_inputs, cancellation_manager)</span>
<span class="g g-Whitespace">   </span><span class="mi">1916</span>         <span class="ow">and</span> <span class="n">executing_eagerly</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1917</span>       <span class="c1"># No tape is watching; skip to running the function.</span>
<span class="ne">-&gt; </span><span class="mi">1918</span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_call_outputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inference_function</span><span class="o">.</span><span class="n">call</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1919</span>           <span class="n">ctx</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">cancellation_manager</span><span class="o">=</span><span class="n">cancellation_manager</span><span class="p">))</span>
<span class="g g-Whitespace">   </span><span class="mi">1920</span>     <span class="n">forward_backward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_select_forward_and_backward_functions</span><span class="p">(</span>

<span class="nn">/usr/local/Caskroom/miniconda/base/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/eager/function.py</span> in <span class="ni">call</span><span class="nt">(self, ctx, args, cancellation_manager)</span>
<span class="g g-Whitespace">    </span><span class="mi">553</span>       <span class="k">with</span> <span class="n">_InterpolateFunctionError</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">554</span>         <span class="k">if</span> <span class="n">cancellation_manager</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">555</span>           <span class="n">outputs</span> <span class="o">=</span> <span class="n">execute</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">556</span>               <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">557</span>               <span class="n">num_outputs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_outputs</span><span class="p">,</span>

<span class="nn">/usr/local/Caskroom/miniconda/base/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/eager/execute.py</span> in <span class="ni">quick_execute</span><span class="nt">(op_name, num_outputs, inputs, attrs, ctx, name)</span>
<span class="g g-Whitespace">     </span><span class="mi">57</span>   <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">58</span>     <span class="n">ctx</span><span class="o">.</span><span class="n">ensure_initialized</span><span class="p">()</span>
<span class="ne">---&gt; </span><span class="mi">59</span>     <span class="n">tensors</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_Py_Execute</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">device_name</span><span class="p">,</span> <span class="n">op_name</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">60</span>                                         <span class="n">inputs</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">61</span>   <span class="k">except</span> <span class="n">core</span><span class="o">.</span><span class="n">_NotOkStatusException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span><span class="o">.</span><span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;verbose&#39;: 1, &#39;epochs&#39;: 30, &#39;steps&#39;: 1719}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dict_keys([&#39;loss&#39;, &#39;accuracy&#39;, &#39;val_loss&#39;, &#39;val_accuracy&#39;])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/101-intro-keras_50_0.png" src="../_images/101-intro-keras_50_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>313/313 [==============================] - 1s 2ms/step - loss: 0.3353 - accuracy: 0.8820
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.3352515995502472, 0.8820000290870667]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_new</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">y_proba</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
<span class="n">y_proba</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96],
       [0.  , 0.  , 0.98, 0.  , 0.02, 0.  , 0.  , 0.  , 0.  , 0.  ],
       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],
      dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[4.1024000e-06, 4.2761562e-07, 7.7352219e-05, 8.9763398e-06,
        3.7400682e-06, 1.1412925e-02, 7.8980111e-06, 2.8233122e-02,
        7.8887114e-04, 9.5946258e-01],
       [1.1116678e-05, 3.6214793e-08, 9.8210180e-01, 1.5544003e-08,
        1.7581107e-02, 1.9387754e-11, 3.0586790e-04, 1.2198677e-11,
        7.3210291e-09, 3.6714097e-11],
       [2.8578486e-06, 9.9999654e-01, 2.0927594e-08, 4.1734179e-07,
        1.2935442e-07, 1.8845613e-13, 5.0816507e-10, 4.4625967e-13,
        8.3641147e-09, 2.6737662e-12]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_pred</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([9, 2, 1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">class_names</span><span class="p">)[</span><span class="n">y_pred</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Ankle boot&#39;, &#39;Pullover&#39;, &#39;Trouser&#39;], dtype=&#39;&lt;U11&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_new</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">y_new</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([9, 2, 1], dtype=uint8)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">X_new</span><span class="p">)):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">class_names</span><span class="p">[</span><span class="n">y_test</span><span class="p">[</span><span class="n">ind</span><span class="p">]])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/101-intro-keras_57_0.png" src="../_images/101-intro-keras_57_0.png" />
</div>
</div>
</div>
<div class="section" id="regression-mlp">
<h1>Regression MLP<a class="headerlink" href="#regression-mlp" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">housing</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>

<span class="n">X_train_full</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train_full</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">housing</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                                                              <span class="n">housing</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                                                              <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_train_full</span><span class="p">,</span>
                                                      <span class="n">y_train_full</span><span class="p">,</span>
                                                      <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_valid</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mean_squared_error&quot;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span>
                    <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
<span class="n">mse_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">X_new</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
363/363 [==============================] - 1s 3ms/step - loss: 2.2656 - val_loss: 0.8560
Epoch 2/20
363/363 [==============================] - 1s 3ms/step - loss: 0.7413 - val_loss: 0.6531
Epoch 3/20
363/363 [==============================] - 1s 2ms/step - loss: 0.6604 - val_loss: 0.6099
Epoch 4/20
363/363 [==============================] - 1s 2ms/step - loss: 0.6245 - val_loss: 0.5658
Epoch 5/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5770 - val_loss: 0.5355
Epoch 6/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5609 - val_loss: 0.5173
Epoch 7/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5500 - val_loss: 0.5081
Epoch 8/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5200 - val_loss: 0.4799
Epoch 9/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5051 - val_loss: 0.4690
Epoch 10/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4910 - val_loss: 0.4656
Epoch 11/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4794 - val_loss: 0.4482
Epoch 12/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4656 - val_loss: 0.4479
Epoch 13/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4693 - val_loss: 0.4296
Epoch 14/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4537 - val_loss: 0.4233
Epoch 15/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4586 - val_loss: 0.4176
Epoch 16/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4612 - val_loss: 0.4123
Epoch 17/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4449 - val_loss: 0.4071
Epoch 18/20
363/363 [==============================] - 0s 1ms/step - loss: 0.4407 - val_loss: 0.4037
Epoch 19/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4184 - val_loss: 0.4000
Epoch 20/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4128 - val_loss: 0.3969
162/162 [==============================] - 0s 1ms/step - loss: 0.4212
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/101-intro-keras_63_0.png" src="../_images/101-intro-keras_63_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.38856643],
       [1.6792021 ],
       [3.1022797 ]], dtype=float32)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="functional-api">
<h1>Functional API<a class="headerlink" href="#functional-api" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">hidden1</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">input_</span><span class="p">)</span>
<span class="n">hidden2</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">hidden1</span><span class="p">)</span>
<span class="n">concat</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">input_</span><span class="p">,</span> <span class="n">hidden2</span><span class="p">])</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">concat</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">input_</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">output</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 8)]          0                                            
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 30)           270         input_1[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 30)           930         dense_5[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 38)           0           input_1[0][0]                    
                                                                 dense_6[0][0]                    
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 1)            39          concatenate[0][0]                
==================================================================================================
Total params: 1,239
Trainable params: 1,239
Non-trainable params: 0
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mean_squared_error&quot;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span>
                    <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
<span class="n">mse_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
363/363 [==============================] - 1s 2ms/step - loss: 1.9731 - val_loss: 3.3940
Epoch 2/20
363/363 [==============================] - 1s 2ms/step - loss: 0.7638 - val_loss: 0.9360
Epoch 3/20
363/363 [==============================] - 1s 2ms/step - loss: 0.6045 - val_loss: 0.5649
Epoch 4/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5862 - val_loss: 0.5712
Epoch 5/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5452 - val_loss: 0.5045
Epoch 6/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5243 - val_loss: 0.4831
Epoch 7/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5185 - val_loss: 0.4639
Epoch 8/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4947 - val_loss: 0.4638
Epoch 9/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4782 - val_loss: 0.4421
Epoch 10/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4708 - val_loss: 0.4313
Epoch 11/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4585 - val_loss: 0.4345
Epoch 12/20
363/363 [==============================] - 0s 1ms/step - loss: 0.4481 - val_loss: 0.4168
Epoch 13/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4476 - val_loss: 0.4230
Epoch 14/20
363/363 [==============================] - 0s 1ms/step - loss: 0.4361 - val_loss: 0.4047
Epoch 15/20
363/363 [==============================] - 1s 1ms/step - loss: 0.4392 - val_loss: 0.4078
Epoch 16/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4420 - val_loss: 0.3938
Epoch 17/20
363/363 [==============================] - 1s 1ms/step - loss: 0.4277 - val_loss: 0.3952
Epoch 18/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4216 - val_loss: 0.3860
Epoch 19/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4033 - val_loss: 0.3827
Epoch 20/20
363/363 [==============================] - 1s 2ms/step - loss: 0.3939 - val_loss: 0.4054
162/162 [==============================] - 0s 3ms/step - loss: 0.4032
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_A</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;wide_input&quot;</span><span class="p">)</span>
<span class="n">input_B</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;deep_input&quot;</span><span class="p">)</span>
<span class="n">hidden1</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">input_B</span><span class="p">)</span>
<span class="n">hidden2</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">hidden1</span><span class="p">)</span>
<span class="n">concat</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">input_A</span><span class="p">,</span> <span class="n">hidden2</span><span class="p">])</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;output&quot;</span><span class="p">)(</span><span class="n">concat</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">input_A</span><span class="p">,</span> <span class="n">input_B</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">output</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>

<span class="n">X_train_A</span><span class="p">,</span> <span class="n">X_train_B</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">5</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:]</span>
<span class="n">X_valid_A</span><span class="p">,</span> <span class="n">X_valid_B</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">5</span><span class="p">],</span> <span class="n">X_valid</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:]</span>
<span class="n">X_test_A</span><span class="p">,</span> <span class="n">X_test_B</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">5</span><span class="p">],</span> <span class="n">X_test</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:]</span>
<span class="n">X_new_A</span><span class="p">,</span> <span class="n">X_new_B</span> <span class="o">=</span> <span class="n">X_test_A</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="n">X_test_B</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">((</span><span class="n">X_train_A</span><span class="p">,</span> <span class="n">X_train_B</span><span class="p">),</span>
                    <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">((</span><span class="n">X_valid_A</span><span class="p">,</span> <span class="n">X_valid_B</span><span class="p">),</span> <span class="n">y_valid</span><span class="p">))</span>
                    
<span class="n">mse_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">((</span><span class="n">X_test_A</span><span class="p">,</span> <span class="n">X_test_B</span><span class="p">),</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">((</span><span class="n">X_new_A</span><span class="p">,</span> <span class="n">X_new_B</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
363/363 [==============================] - 2s 3ms/step - loss: 3.1941 - val_loss: 0.8072
Epoch 2/20
363/363 [==============================] - 1s 3ms/step - loss: 0.7247 - val_loss: 0.6658
Epoch 3/20
363/363 [==============================] - 0s 1ms/step - loss: 0.6176 - val_loss: 0.5687
Epoch 4/20
363/363 [==============================] - 0s 1ms/step - loss: 0.5799 - val_loss: 0.5296
Epoch 5/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5409 - val_loss: 0.4993
Epoch 6/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5173 - val_loss: 0.4811
Epoch 7/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5186 - val_loss: 0.4696
Epoch 8/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4977 - val_loss: 0.4496
Epoch 9/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4765 - val_loss: 0.4404
Epoch 10/20
363/363 [==============================] - 1s 1ms/step - loss: 0.4676 - val_loss: 0.4315
Epoch 11/20
363/363 [==============================] - 0s 1ms/step - loss: 0.4574 - val_loss: 0.4268
Epoch 12/20
363/363 [==============================] - 0s 1ms/step - loss: 0.4479 - val_loss: 0.4166
Epoch 13/20
363/363 [==============================] - 0s 1ms/step - loss: 0.4487 - val_loss: 0.4125
Epoch 14/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4469 - val_loss: 0.4074
Epoch 15/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4460 - val_loss: 0.4044
Epoch 16/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4495 - val_loss: 0.4007
Epoch 17/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4378 - val_loss: 0.4013
Epoch 18/20
363/363 [==============================] - 0s 1ms/step - loss: 0.4375 - val_loss: 0.3987
Epoch 19/20
363/363 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.3934
Epoch 20/20
363/363 [==============================] - 1s 2ms/step - loss: 0.4078 - val_loss: 0.4204
162/162 [==============================] - 0s 2ms/step - loss: 0.4219
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_A</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;wide_input&quot;</span><span class="p">)</span>
<span class="n">input_B</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;deep_input&quot;</span><span class="p">)</span>
<span class="n">hidden1</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">input_B</span><span class="p">)</span>
<span class="n">hidden2</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">hidden1</span><span class="p">)</span>
<span class="n">concat</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">input_A</span><span class="p">,</span> <span class="n">hidden2</span><span class="p">])</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;main_output&quot;</span><span class="p">)(</span><span class="n">concat</span><span class="p">)</span>
<span class="n">aux_output</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;aux_output&quot;</span><span class="p">)(</span><span class="n">hidden2</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">input_A</span><span class="p">,</span> <span class="n">input_B</span><span class="p">],</span>
                           <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">output</span><span class="p">,</span> <span class="n">aux_output</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="s2">&quot;mse&quot;</span><span class="p">],</span> <span class="n">loss_weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">X_train_A</span><span class="p">,</span> <span class="n">X_train_B</span><span class="p">],</span> <span class="p">[</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">],</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">([</span><span class="n">X_valid_A</span><span class="p">,</span> <span class="n">X_valid_B</span><span class="p">],</span> <span class="p">[</span><span class="n">y_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
363/363 [==============================] - 2s 3ms/step - loss: 3.4633 - main_output_loss: 3.3289 - aux_output_loss: 4.6732 - val_loss: 1.6233 - val_main_output_loss: 0.8468 - val_aux_output_loss: 8.6117
Epoch 2/20
363/363 [==============================] - 1s 3ms/step - loss: 0.9807 - main_output_loss: 0.7503 - aux_output_loss: 3.0537 - val_loss: 1.5163 - val_main_output_loss: 0.6836 - val_aux_output_loss: 9.0109
Epoch 3/20
363/363 [==============================] - 1s 2ms/step - loss: 0.7742 - main_output_loss: 0.6290 - aux_output_loss: 2.0810 - val_loss: 1.4639 - val_main_output_loss: 0.6229 - val_aux_output_loss: 9.0326
Epoch 4/20
363/363 [==============================] - 1s 2ms/step - loss: 0.6952 - main_output_loss: 0.5897 - aux_output_loss: 1.6449 - val_loss: 1.3388 - val_main_output_loss: 0.5481 - val_aux_output_loss: 8.4552
Epoch 5/20
363/363 [==============================] - 1s 3ms/step - loss: 0.6469 - main_output_loss: 0.5508 - aux_output_loss: 1.5118 - val_loss: 1.2177 - val_main_output_loss: 0.5194 - val_aux_output_loss: 7.5030
Epoch 6/20
363/363 [==============================] - 1s 3ms/step - loss: 0.6120 - main_output_loss: 0.5251 - aux_output_loss: 1.3943 - val_loss: 1.0935 - val_main_output_loss: 0.5106 - val_aux_output_loss: 6.3396
Epoch 7/20
363/363 [==============================] - 1s 2ms/step - loss: 0.6114 - main_output_loss: 0.5256 - aux_output_loss: 1.3833 - val_loss: 0.9918 - val_main_output_loss: 0.5115 - val_aux_output_loss: 5.3151
Epoch 8/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5765 - main_output_loss: 0.5024 - aux_output_loss: 1.2439 - val_loss: 0.8733 - val_main_output_loss: 0.4733 - val_aux_output_loss: 4.4740
Epoch 9/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5535 - main_output_loss: 0.4811 - aux_output_loss: 1.2057 - val_loss: 0.7832 - val_main_output_loss: 0.4555 - val_aux_output_loss: 3.7323
Epoch 10/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5456 - main_output_loss: 0.4708 - aux_output_loss: 1.2189 - val_loss: 0.7170 - val_main_output_loss: 0.4604 - val_aux_output_loss: 3.0262
Epoch 11/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5297 - main_output_loss: 0.4587 - aux_output_loss: 1.1684 - val_loss: 0.6510 - val_main_output_loss: 0.4293 - val_aux_output_loss: 2.6468
Epoch 12/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5181 - main_output_loss: 0.4501 - aux_output_loss: 1.1305 - val_loss: 0.6051 - val_main_output_loss: 0.4310 - val_aux_output_loss: 2.1722
Epoch 13/20
363/363 [==============================] - 1s 4ms/step - loss: 0.5100 - main_output_loss: 0.4487 - aux_output_loss: 1.0620 - val_loss: 0.5644 - val_main_output_loss: 0.4161 - val_aux_output_loss: 1.8992
Epoch 14/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5064 - main_output_loss: 0.4459 - aux_output_loss: 1.0503 - val_loss: 0.5354 - val_main_output_loss: 0.4119 - val_aux_output_loss: 1.6466
Epoch 15/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5027 - main_output_loss: 0.4452 - aux_output_loss: 1.0207 - val_loss: 0.5124 - val_main_output_loss: 0.4047 - val_aux_output_loss: 1.4812
Epoch 16/20
363/363 [==============================] - 1s 2ms/step - loss: 0.5057 - main_output_loss: 0.4480 - aux_output_loss: 1.0249 - val_loss: 0.4934 - val_main_output_loss: 0.4034 - val_aux_output_loss: 1.3035
Epoch 17/20
363/363 [==============================] - 0s 1ms/step - loss: 0.4931 - main_output_loss: 0.4360 - aux_output_loss: 1.0075 - val_loss: 0.4801 - val_main_output_loss: 0.3984 - val_aux_output_loss: 1.2150
Epoch 18/20
363/363 [==============================] - 1s 1ms/step - loss: 0.4922 - main_output_loss: 0.4352 - aux_output_loss: 1.0053 - val_loss: 0.4694 - val_main_output_loss: 0.3962 - val_aux_output_loss: 1.1279
Epoch 19/20
363/363 [==============================] - 1s 1ms/step - loss: 0.4658 - main_output_loss: 0.4139 - aux_output_loss: 0.9323 - val_loss: 0.4580 - val_main_output_loss: 0.3936 - val_aux_output_loss: 1.0372
Epoch 20/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4589 - main_output_loss: 0.4072 - aux_output_loss: 0.9243 - val_loss: 0.4655 - val_main_output_loss: 0.4048 - val_aux_output_loss: 1.0118
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">total_loss</span><span class="p">,</span> <span class="n">main_loss</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="p">[</span><span class="n">X_test_A</span><span class="p">,</span> <span class="n">X_test_B</span><span class="p">],</span> <span class="p">[</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">])</span>
<span class="n">y_pred_main</span><span class="p">,</span> <span class="n">y_pred_aux</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">X_new_A</span><span class="p">,</span> <span class="n">X_new_B</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>162/162 [==============================] - 0s 991us/step - loss: 0.4668 - main_output_loss: 0.4178 - aux_output_loss: 0.9082
WARNING:tensorflow:5 out of the last 7 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x1407f5430&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="the-subclassing-api">
<h1>The subclassing API<a class="headerlink" href="#the-subclassing-api" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">WideAndDeepModel</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden1</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden2</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">main_output</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aux_output</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">input_A</span><span class="p">,</span> <span class="n">input_B</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">hidden1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden1</span><span class="p">(</span><span class="n">input_B</span><span class="p">)</span>
        <span class="n">hidden2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden2</span><span class="p">(</span><span class="n">hidden1</span><span class="p">)</span>
        <span class="n">concat</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">input_A</span><span class="p">,</span> <span class="n">hidden2</span><span class="p">])</span>
        <span class="n">main_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">main_output</span><span class="p">(</span><span class="n">concat</span><span class="p">)</span>
        <span class="n">aux_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_output</span><span class="p">(</span><span class="n">hidden2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">main_output</span><span class="p">,</span> <span class="n">aux_output</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">WideAndDeepModel</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span>
              <span class="n">loss_weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="p">(</span><span class="n">X_train_A</span><span class="p">,</span> <span class="n">X_train_B</span><span class="p">),</span> <span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">((</span><span class="n">X_valid_A</span><span class="p">,</span> <span class="n">X_valid_B</span><span class="p">),</span> <span class="p">(</span><span class="n">y_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)))</span>
<span class="n">total_loss</span><span class="p">,</span> <span class="n">main_loss</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">((</span><span class="n">X_test_A</span><span class="p">,</span> <span class="n">X_test_B</span><span class="p">),</span>
                                                 <span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="n">y_pred_main</span><span class="p">,</span> <span class="n">y_pred_aux</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">((</span><span class="n">X_new_A</span><span class="p">,</span> <span class="n">X_new_B</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
363/363 [==============================] - 1s 2ms/step - loss: 3.3855 - output_1_loss: 3.3304 - output_2_loss: 3.8821 - val_loss: 2.1435 - val_output_1_loss: 1.1581 - val_output_2_loss: 11.0117
Epoch 2/10
363/363 [==============================] - 1s 2ms/step - loss: 1.0790 - output_1_loss: 0.9329 - output_2_loss: 2.3942 - val_loss: 1.7567 - val_output_1_loss: 0.8205 - val_output_2_loss: 10.1825
Epoch 3/10
363/363 [==============================] - 1s 2ms/step - loss: 0.8644 - output_1_loss: 0.7583 - output_2_loss: 1.8194 - val_loss: 1.5664 - val_output_1_loss: 0.7913 - val_output_2_loss: 8.5419
Epoch 4/10
363/363 [==============================] - 1s 2ms/step - loss: 0.7850 - output_1_loss: 0.6979 - output_2_loss: 1.5689 - val_loss: 1.3088 - val_output_1_loss: 0.6549 - val_output_2_loss: 7.1933
Epoch 5/10
363/363 [==============================] - 1s 2ms/step - loss: 0.7294 - output_1_loss: 0.6499 - output_2_loss: 1.4452 - val_loss: 1.1357 - val_output_1_loss: 0.5964 - val_output_2_loss: 5.9898
Epoch 6/10
363/363 [==============================] - 1s 1ms/step - loss: 0.6880 - output_1_loss: 0.6092 - output_2_loss: 1.3974 - val_loss: 1.0036 - val_output_1_loss: 0.5937 - val_output_2_loss: 4.6933
Epoch 7/10
363/363 [==============================] - 1s 2ms/step - loss: 0.6918 - output_1_loss: 0.6143 - output_2_loss: 1.3899 - val_loss: 0.8904 - val_output_1_loss: 0.5591 - val_output_2_loss: 3.8714
Epoch 8/10
363/363 [==============================] - 1s 2ms/step - loss: 0.6504 - output_1_loss: 0.5805 - output_2_loss: 1.2797 - val_loss: 0.8009 - val_output_1_loss: 0.5243 - val_output_2_loss: 3.2903
Epoch 9/10
363/363 [==============================] - 1s 1ms/step - loss: 0.6270 - output_1_loss: 0.5574 - output_2_loss: 1.2533 - val_loss: 0.7357 - val_output_1_loss: 0.5144 - val_output_2_loss: 2.7275
Epoch 10/10
363/363 [==============================] - 1s 1ms/step - loss: 0.6160 - output_1_loss: 0.5456 - output_2_loss: 1.2495 - val_loss: 0.6849 - val_output_1_loss: 0.5014 - val_output_2_loss: 2.3370
162/162 [==============================] - 0s 876us/step - loss: 0.5841 - output_1_loss: 0.5188 - output_2_loss: 1.1722
WARNING:tensorflow:6 out of the last 8 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x140c63a60&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="saving-and-restoring">
<h1>Saving and Restoring<a class="headerlink" href="#saving-and-restoring" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">]),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
<span class="n">mse_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
363/363 [==============================] - 2s 3ms/step - loss: 3.3697 - val_loss: 0.7126
Epoch 2/10
363/363 [==============================] - 1s 3ms/step - loss: 0.6964 - val_loss: 0.6880
Epoch 3/10
363/363 [==============================] - 1s 2ms/step - loss: 0.6167 - val_loss: 0.5803
Epoch 4/10
363/363 [==============================] - 1s 2ms/step - loss: 0.5846 - val_loss: 0.5166
Epoch 5/10
363/363 [==============================] - 1s 2ms/step - loss: 0.5321 - val_loss: 0.4895
Epoch 6/10
363/363 [==============================] - 1s 2ms/step - loss: 0.5083 - val_loss: 0.4951
Epoch 7/10
363/363 [==============================] - 1s 2ms/step - loss: 0.5044 - val_loss: 0.4861
Epoch 8/10
363/363 [==============================] - 1s 2ms/step - loss: 0.4813 - val_loss: 0.4554
Epoch 9/10
363/363 [==============================] - 0s 1ms/step - loss: 0.4627 - val_loss: 0.4413
Epoch 10/10
363/363 [==============================] - 0s 1ms/step - loss: 0.4549 - val_loss: 0.4379
162/162 [==============================] - 0s 796us/step - loss: 0.4382
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;data/101/my_keras_model.h5&quot;</span>
<span class="c1"># model.save(path)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:7 out of the last 9 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x140d21280&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.5400237],
       [1.6505971],
       [3.0098243]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># model.save_weights(&quot;data/101/my_keras_weights.ckpt&quot;)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s2">&quot;data/101/my_keras_weights.ckpt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x140df4fa0&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="using-callbacks-during-training">
<h1>Using Callbacks during Training<a class="headerlink" href="#using-callbacks-during-training" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">]),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="p">])</span>    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="n">checkpoint_cb</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s2">&quot;data/101/my_keras_model2.h5&quot;</span><span class="p">,</span> <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint_cb</span><span class="p">])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="s2">&quot;data/101/my_keras_model2.h5&quot;</span><span class="p">)</span>  <span class="c1"># rollback to best model</span>
<span class="n">mse_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
363/363 [==============================] - 1s 2ms/step - loss: 3.3697 - val_loss: 0.7126
Epoch 2/10
363/363 [==============================] - 1s 3ms/step - loss: 0.6964 - val_loss: 0.6880
Epoch 3/10
363/363 [==============================] - 1s 4ms/step - loss: 0.6167 - val_loss: 0.5803
Epoch 4/10
363/363 [==============================] - 1s 2ms/step - loss: 0.5846 - val_loss: 0.5166
Epoch 5/10
363/363 [==============================] - 1s 3ms/step - loss: 0.5321 - val_loss: 0.4895
Epoch 6/10
363/363 [==============================] - 1s 4ms/step - loss: 0.5083 - val_loss: 0.4951
Epoch 7/10
363/363 [==============================] - 1s 3ms/step - loss: 0.5044 - val_loss: 0.4861
Epoch 8/10
363/363 [==============================] - 1s 3ms/step - loss: 0.4813 - val_loss: 0.4554
Epoch 9/10
363/363 [==============================] - 1s 4ms/step - loss: 0.4627 - val_loss: 0.4413
Epoch 10/10
363/363 [==============================] - 1s 3ms/step - loss: 0.4549 - val_loss: 0.4379
162/162 [==============================] - 1s 3ms/step - loss: 0.4382
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="n">early_stopping_cb</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                                  <span class="n">restore_best_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint_cb</span><span class="p">,</span> <span class="n">early_stopping_cb</span><span class="p">])</span>
<span class="n">mse_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/100
363/363 [==============================] - 1s 3ms/step - loss: 0.4578 - val_loss: 0.4110
Epoch 2/100
363/363 [==============================] - 1s 2ms/step - loss: 0.4430 - val_loss: 0.4266
Epoch 3/100
363/363 [==============================] - 1s 2ms/step - loss: 0.4376 - val_loss: 0.3996
Epoch 4/100
363/363 [==============================] - 0s 1ms/step - loss: 0.4361 - val_loss: 0.3939
Epoch 5/100
363/363 [==============================] - 0s 1ms/step - loss: 0.4204 - val_loss: 0.3889
Epoch 6/100
363/363 [==============================] - 0s 1ms/step - loss: 0.4112 - val_loss: 0.3866
Epoch 7/100
363/363 [==============================] - 0s 1ms/step - loss: 0.4226 - val_loss: 0.3860
Epoch 8/100
363/363 [==============================] - 1s 1ms/step - loss: 0.4135 - val_loss: 0.3793
Epoch 9/100
363/363 [==============================] - 1s 2ms/step - loss: 0.4039 - val_loss: 0.3746
Epoch 10/100
363/363 [==============================] - 1s 3ms/step - loss: 0.4023 - val_loss: 0.3723
Epoch 11/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3950 - val_loss: 0.3697
Epoch 12/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3912 - val_loss: 0.3669
Epoch 13/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3939 - val_loss: 0.3661
Epoch 14/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3868 - val_loss: 0.3631
Epoch 15/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3878 - val_loss: 0.3660
Epoch 16/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3935 - val_loss: 0.3625
Epoch 17/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.3592
Epoch 18/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3801 - val_loss: 0.3563
Epoch 19/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3679 - val_loss: 0.3535
Epoch 20/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.3709
Epoch 21/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3746 - val_loss: 0.3512
Epoch 22/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3605 - val_loss: 0.3699
Epoch 23/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3822 - val_loss: 0.3476
Epoch 24/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3626 - val_loss: 0.3561
Epoch 25/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3610 - val_loss: 0.3527
Epoch 26/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3626 - val_loss: 0.3700
Epoch 27/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3685 - val_loss: 0.3432
Epoch 28/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3684 - val_loss: 0.3592
Epoch 29/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.3521
Epoch 30/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3687 - val_loss: 0.3626
Epoch 31/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3613 - val_loss: 0.3431
Epoch 32/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.3765
Epoch 33/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3620 - val_loss: 0.3374
Epoch 34/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3502 - val_loss: 0.3407
Epoch 35/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3614
Epoch 36/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3348
Epoch 37/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3780 - val_loss: 0.3573
Epoch 38/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3367
Epoch 39/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3689 - val_loss: 0.3425
Epoch 40/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3485 - val_loss: 0.3369
Epoch 41/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3675 - val_loss: 0.3515
Epoch 42/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3426
Epoch 43/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.3677
Epoch 44/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3407 - val_loss: 0.3564
Epoch 45/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3554 - val_loss: 0.3336
Epoch 46/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3499 - val_loss: 0.3457
Epoch 47/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3623 - val_loss: 0.3433
Epoch 48/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3401 - val_loss: 0.3659
Epoch 49/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.3286
Epoch 50/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.3268
Epoch 51/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3439
Epoch 52/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3405 - val_loss: 0.3263
Epoch 53/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3910
Epoch 54/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3337 - val_loss: 0.3275
Epoch 55/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3462 - val_loss: 0.3561
Epoch 56/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3342 - val_loss: 0.3237
Epoch 57/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3395 - val_loss: 0.3242
Epoch 58/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3315 - val_loss: 0.3765
Epoch 59/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3394 - val_loss: 0.3289
Epoch 60/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3378 - val_loss: 0.3502
Epoch 61/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3522 - val_loss: 0.3456
Epoch 62/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3473 - val_loss: 0.3445
Epoch 63/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3427 - val_loss: 0.3290
Epoch 64/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3212 - val_loss: 0.3217
Epoch 65/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3374 - val_loss: 0.3351
Epoch 66/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3323 - val_loss: 0.3232
Epoch 67/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3566
Epoch 68/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3316 - val_loss: 0.3257
Epoch 69/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3354 - val_loss: 0.3348
Epoch 70/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3316 - val_loss: 0.3560
Epoch 71/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3371 - val_loss: 0.3583
Epoch 72/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3201 - val_loss: 0.3287
Epoch 73/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3373 - val_loss: 0.3203
Epoch 74/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3327 - val_loss: 0.3840
Epoch 75/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3268 - val_loss: 0.3233
Epoch 76/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3322 - val_loss: 0.3476
Epoch 77/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3224 - val_loss: 0.3407
Epoch 78/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3331 - val_loss: 0.3462
Epoch 79/100
363/363 [==============================] - 2s 4ms/step - loss: 0.3310 - val_loss: 0.3347
Epoch 80/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3323 - val_loss: 0.3354
Epoch 81/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3297 - val_loss: 0.3274
Epoch 82/100
363/363 [==============================] - 1s 4ms/step - loss: 0.3441 - val_loss: 0.3167
Epoch 83/100
363/363 [==============================] - 2s 5ms/step - loss: 0.3369 - val_loss: 0.3280
Epoch 84/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3182 - val_loss: 0.3634
Epoch 85/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3235 - val_loss: 0.3176
Epoch 86/100
363/363 [==============================] - 1s 4ms/step - loss: 0.3184 - val_loss: 0.3156
Epoch 87/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3395 - val_loss: 0.3529
Epoch 88/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3264 - val_loss: 0.3258
Epoch 89/100
363/363 [==============================] - 2s 5ms/step - loss: 0.3210 - val_loss: 0.3630
Epoch 90/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3192 - val_loss: 0.3376
Epoch 91/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3237 - val_loss: 0.3211
Epoch 92/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3281 - val_loss: 0.3456
Epoch 93/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3424 - val_loss: 0.3158
Epoch 94/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3209 - val_loss: 0.3409
Epoch 95/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3230 - val_loss: 0.3379
Epoch 96/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3341 - val_loss: 0.3213
162/162 [==============================] - 0s 3ms/step - loss: 0.3310
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PrintValTrainRatioCallback</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">logs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">val/train: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">logs</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">val_train_ratio_cb</span> <span class="o">=</span> <span class="n">PrintValTrainRatioCallback</span><span class="p">()</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">val_train_ratio_cb</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>363/363 [==============================] - 1s 2ms/step - loss: 0.3302 - val_loss: 0.3556

val/train: 1.08
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="tensorboard">
<h1>TensorBoard<a class="headerlink" href="#tensorboard" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">root_logdir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">curdir</span><span class="p">,</span> <span class="s2">&quot;my_logs&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_run_logdir</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">time</span>
    <span class="n">run_id</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;run_%Y_%m_</span><span class="si">%d</span><span class="s2">-%H_%M_%S&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root_logdir</span><span class="p">,</span> <span class="n">run_id</span><span class="p">)</span>

<span class="n">run_logdir</span> <span class="o">=</span> <span class="n">get_run_logdir</span><span class="p">()</span>
<span class="n">run_logdir</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;./my_logs/run_2021_09_15-16_40_15&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">]),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tensorboard_cb</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">run_logdir</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint_cb</span><span class="p">,</span> <span class="n">tensorboard_cb</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2021-09-15 16:40:16.040831: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-09-15 16:40:16.040855: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-09-15 16:40:16.043464: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/30
 32/363 [=&gt;............................] - ETA: 2s - loss: 6.6230
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2021-09-15 16:40:16.510777: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-09-15 16:40:16.510796: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-09-15 16:40:16.516666: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-09-15 16:40:16.528767: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-09-15 16:40:16.548736: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ./my_logs/run_2021_09_15-16_40_15/train/plugins/profile/2021_09_15_16_40_16
2021-09-15 16:40:16.550136: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to ./my_logs/run_2021_09_15-16_40_15/train/plugins/profile/2021_09_15_16_40_16/Xavier-Yang.local.trace.json.gz
2021-09-15 16:40:16.579954: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ./my_logs/run_2021_09_15-16_40_15/train/plugins/profile/2021_09_15_16_40_16
2021-09-15 16:40:16.580225: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to ./my_logs/run_2021_09_15-16_40_15/train/plugins/profile/2021_09_15_16_40_16/Xavier-Yang.local.memory_profile.json.gz
2021-09-15 16:40:16.582095: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./my_logs/run_2021_09_15-16_40_15/train/plugins/profile/2021_09_15_16_40_16Dumped tool data for xplane.pb to ./my_logs/run_2021_09_15-16_40_15/train/plugins/profile/2021_09_15_16_40_16/Xavier-Yang.local.xplane.pb
Dumped tool data for overview_page.pb to ./my_logs/run_2021_09_15-16_40_15/train/plugins/profile/2021_09_15_16_40_16/Xavier-Yang.local.overview_page.pb
Dumped tool data for input_pipeline.pb to ./my_logs/run_2021_09_15-16_40_15/train/plugins/profile/2021_09_15_16_40_16/Xavier-Yang.local.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to ./my_logs/run_2021_09_15-16_40_15/train/plugins/profile/2021_09_15_16_40_16/Xavier-Yang.local.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to ./my_logs/run_2021_09_15-16_40_15/train/plugins/profile/2021_09_15_16_40_16/Xavier-Yang.local.kernel_stats.pb
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>363/363 [==============================] - 1s 3ms/step - loss: 3.3697 - val_loss: 0.7126
Epoch 2/30
363/363 [==============================] - 1s 2ms/step - loss: 0.6964 - val_loss: 0.6880
Epoch 3/30
363/363 [==============================] - 1s 2ms/step - loss: 0.6167 - val_loss: 0.5803
Epoch 4/30
363/363 [==============================] - 1s 2ms/step - loss: 0.5846 - val_loss: 0.5166
Epoch 5/30
363/363 [==============================] - 1s 2ms/step - loss: 0.5321 - val_loss: 0.4895
Epoch 6/30
363/363 [==============================] - 1s 2ms/step - loss: 0.5083 - val_loss: 0.4951
Epoch 7/30
363/363 [==============================] - 1s 3ms/step - loss: 0.5044 - val_loss: 0.4861
Epoch 8/30
363/363 [==============================] - 0s 1ms/step - loss: 0.4813 - val_loss: 0.4554
Epoch 9/30
363/363 [==============================] - 1s 2ms/step - loss: 0.4627 - val_loss: 0.4413
Epoch 10/30
363/363 [==============================] - 1s 2ms/step - loss: 0.4549 - val_loss: 0.4379
Epoch 11/30
363/363 [==============================] - 1s 2ms/step - loss: 0.4416 - val_loss: 0.4396
Epoch 12/30
363/363 [==============================] - 1s 2ms/step - loss: 0.4295 - val_loss: 0.4507
Epoch 13/30
363/363 [==============================] - 1s 2ms/step - loss: 0.4326 - val_loss: 0.3997
Epoch 14/30
363/363 [==============================] - 0s 1ms/step - loss: 0.4207 - val_loss: 0.3956
Epoch 15/30
363/363 [==============================] - 1s 2ms/step - loss: 0.4198 - val_loss: 0.3916
Epoch 16/30
363/363 [==============================] - 1s 2ms/step - loss: 0.4248 - val_loss: 0.3937
Epoch 17/30
363/363 [==============================] - 1s 2ms/step - loss: 0.4105 - val_loss: 0.3809
Epoch 18/30
363/363 [==============================] - 1s 2ms/step - loss: 0.4070 - val_loss: 0.3793
Epoch 19/30
363/363 [==============================] - 1s 2ms/step - loss: 0.3902 - val_loss: 0.3850
Epoch 20/30
363/363 [==============================] - 1s 2ms/step - loss: 0.3864 - val_loss: 0.3809
Epoch 21/30
363/363 [==============================] - 1s 2ms/step - loss: 0.3978 - val_loss: 0.3701
Epoch 22/30
363/363 [==============================] - 0s 1ms/step - loss: 0.3816 - val_loss: 0.3781
Epoch 23/30
363/363 [==============================] - 0s 1ms/step - loss: 0.4042 - val_loss: 0.3650
Epoch 24/30
363/363 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.3655
Epoch 25/30
363/363 [==============================] - 1s 1ms/step - loss: 0.3792 - val_loss: 0.3611
Epoch 26/30
363/363 [==============================] - 0s 1ms/step - loss: 0.3800 - val_loss: 0.3626
Epoch 27/30
363/363 [==============================] - 0s 1ms/step - loss: 0.3858 - val_loss: 0.3564
Epoch 28/30
363/363 [==============================] - 1s 3ms/step - loss: 0.3839 - val_loss: 0.3579
Epoch 29/30
363/363 [==============================] - 1s 3ms/step - loss: 0.3736 - val_loss: 0.3561
Epoch 30/30
363/363 [==============================] - 1s 3ms/step - loss: 0.3843 - val_loss: 0.3548
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">]),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="p">])</span>    
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">run_logdir2</span> <span class="o">=</span> <span class="n">get_run_logdir</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tensorboard_cb</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">run_logdir2</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint_cb</span><span class="p">,</span> <span class="n">tensorboard_cb</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2021-09-15 16:40:37.443638: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-09-15 16:40:37.443695: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-09-15 16:40:37.443738: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/30
 49/363 [===&gt;..........................] - ETA: 1s - loss: 1.6313
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2021-09-15 16:40:38.491509: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-09-15 16:40:38.491591: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-09-15 16:40:38.508847: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-09-15 16:40:38.512032: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-09-15 16:40:38.514121: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ./my_logs/run_2021_09_15-16_40_37/train/plugins/profile/2021_09_15_16_40_38
2021-09-15 16:40:38.516878: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to ./my_logs/run_2021_09_15-16_40_37/train/plugins/profile/2021_09_15_16_40_38/Xavier-Yang.local.trace.json.gz
2021-09-15 16:40:38.519447: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ./my_logs/run_2021_09_15-16_40_37/train/plugins/profile/2021_09_15_16_40_38
2021-09-15 16:40:38.519895: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to ./my_logs/run_2021_09_15-16_40_37/train/plugins/profile/2021_09_15_16_40_38/Xavier-Yang.local.memory_profile.json.gz
2021-09-15 16:40:38.521261: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./my_logs/run_2021_09_15-16_40_37/train/plugins/profile/2021_09_15_16_40_38Dumped tool data for xplane.pb to ./my_logs/run_2021_09_15-16_40_37/train/plugins/profile/2021_09_15_16_40_38/Xavier-Yang.local.xplane.pb
Dumped tool data for overview_page.pb to ./my_logs/run_2021_09_15-16_40_37/train/plugins/profile/2021_09_15_16_40_38/Xavier-Yang.local.overview_page.pb
Dumped tool data for input_pipeline.pb to ./my_logs/run_2021_09_15-16_40_37/train/plugins/profile/2021_09_15_16_40_38/Xavier-Yang.local.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to ./my_logs/run_2021_09_15-16_40_37/train/plugins/profile/2021_09_15_16_40_38/Xavier-Yang.local.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to ./my_logs/run_2021_09_15-16_40_37/train/plugins/profile/2021_09_15_16_40_38/Xavier-Yang.local.kernel_stats.pb
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>363/363 [==============================] - 3s 5ms/step - loss: 0.7645 - val_loss: 302.8536
Epoch 2/30
363/363 [==============================] - 1s 2ms/step - loss: 8159520618.2209 - val_loss: 1.3230
Epoch 3/30
363/363 [==============================] - 1s 3ms/step - loss: 1.3439 - val_loss: 1.3176
Epoch 4/30
363/363 [==============================] - 0s 1ms/step - loss: 1.3546 - val_loss: 1.3261
Epoch 5/30
363/363 [==============================] - 1s 2ms/step - loss: 1.3513 - val_loss: 1.3154
Epoch 6/30
363/363 [==============================] - 1s 2ms/step - loss: 1.3274 - val_loss: 1.3203
Epoch 7/30
363/363 [==============================] - 1s 3ms/step - loss: 1.3639 - val_loss: 1.3149
Epoch 8/30
363/363 [==============================] - 1s 3ms/step - loss: 1.3487 - val_loss: 1.3157
Epoch 9/30
363/363 [==============================] - 1s 2ms/step - loss: 1.3445 - val_loss: 1.3150
Epoch 10/30
363/363 [==============================] - 1s 2ms/step - loss: 1.3697 - val_loss: 1.3172
Epoch 11/30
363/363 [==============================] - 1s 2ms/step - loss: 1.3622 - val_loss: 1.3174
Epoch 12/30
363/363 [==============================] - 1s 3ms/step - loss: 1.3389 - val_loss: 1.3150
Epoch 13/30
363/363 [==============================] - 1s 3ms/step - loss: 1.3336 - val_loss: 1.3270
Epoch 14/30
363/363 [==============================] - 1s 2ms/step - loss: 1.3429 - val_loss: 1.3195
Epoch 15/30
363/363 [==============================] - 1s 3ms/step - loss: 1.3275 - val_loss: 1.3157
Epoch 16/30
363/363 [==============================] - 1s 2ms/step - loss: 1.3669 - val_loss: 1.3182
Epoch 17/30
363/363 [==============================] - 1s 3ms/step - loss: 1.3645 - val_loss: 1.3223
Epoch 18/30
363/363 [==============================] - 0s 1ms/step - loss: 1.3839 - val_loss: 1.3154
Epoch 19/30
363/363 [==============================] - 1s 2ms/step - loss: 1.3078 - val_loss: 1.3168
Epoch 20/30
363/363 [==============================] - 1s 2ms/step - loss: 1.3215 - val_loss: 1.3151
Epoch 21/30
363/363 [==============================] - 1s 3ms/step - loss: 1.3344 - val_loss: 1.3174
Epoch 22/30
363/363 [==============================] - 1s 3ms/step - loss: 1.3269 - val_loss: 1.3204
Epoch 23/30
363/363 [==============================] - 1s 2ms/step - loss: 1.3590 - val_loss: 1.3164
Epoch 24/30
363/363 [==============================] - 1s 3ms/step - loss: 1.3381 - val_loss: 1.3157
Epoch 25/30
363/363 [==============================] - 0s 1ms/step - loss: 1.3265 - val_loss: 1.3180
Epoch 26/30
363/363 [==============================] - 0s 1ms/step - loss: 1.3532 - val_loss: 1.3195
Epoch 27/30
363/363 [==============================] - 0s 1ms/step - loss: 1.3552 - val_loss: 1.3157
Epoch 28/30
363/363 [==============================] - 0s 1ms/step - loss: 1.3447 - val_loss: 1.3222
Epoch 29/30
363/363 [==============================] - 1s 2ms/step - loss: 1.3379 - val_loss: 1.3267
Epoch 30/30
363/363 [==============================] - 1s 1ms/step - loss: 1.3583 - val_loss: 1.3174
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">help</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">TensorBoard</span><span class="o">.</span><span class="fm">__init__</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Help on function __init__ in module tensorflow.python.keras.callbacks:

__init__(self, log_dir=&#39;logs&#39;, histogram_freq=0, write_graph=True, write_images=False, update_freq=&#39;epoch&#39;, profile_batch=2, embeddings_freq=0, embeddings_metadata=None, **kwargs)
    Initialize self.  See help(type(self)) for accurate signature.
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="hyperparameter-tuning">
<h1>Hyperparameter Tuning<a class="headerlink" href="#hyperparameter-tuning" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span><span class="n">n_hidden</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_neurons</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">]):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras_reg</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">wrappers</span><span class="o">.</span><span class="n">scikit_learn</span><span class="o">.</span><span class="n">KerasRegressor</span><span class="p">(</span><span class="n">build_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
              <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
              <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/100
363/363 [==============================] - 1s 2ms/step - loss: 1.5673 - val_loss: 20.7721
Epoch 2/100
363/363 [==============================] - 1s 2ms/step - loss: 1.3216 - val_loss: 5.0266
Epoch 3/100
363/363 [==============================] - 1s 2ms/step - loss: 0.5972 - val_loss: 0.5490
Epoch 4/100
363/363 [==============================] - 1s 4ms/step - loss: 0.4985 - val_loss: 0.4529
Epoch 5/100
363/363 [==============================] - 1s 2ms/step - loss: 0.4608 - val_loss: 0.4188
Epoch 6/100
363/363 [==============================] - 1s 2ms/step - loss: 0.4410 - val_loss: 0.4129
Epoch 7/100
363/363 [==============================] - 1s 3ms/step - loss: 0.4463 - val_loss: 0.4004
Epoch 8/100
363/363 [==============================] - 1s 3ms/step - loss: 0.4283 - val_loss: 0.3944
Epoch 9/100
363/363 [==============================] - 1s 2ms/step - loss: 0.4139 - val_loss: 0.3961
Epoch 10/100
363/363 [==============================] - 1s 2ms/step - loss: 0.4107 - val_loss: 0.4071
Epoch 11/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3992 - val_loss: 0.3855
Epoch 12/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3982 - val_loss: 0.4136
Epoch 13/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.3997
Epoch 14/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3910 - val_loss: 0.3818
Epoch 15/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3948 - val_loss: 0.3829
Epoch 16/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3981 - val_loss: 0.3739
Epoch 17/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3821 - val_loss: 0.4022
Epoch 18/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3851 - val_loss: 0.3873
Epoch 19/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3753 - val_loss: 0.3768
Epoch 20/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3634 - val_loss: 0.4191
Epoch 21/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3787 - val_loss: 0.3927
Epoch 22/100
363/363 [==============================] - 1s 4ms/step - loss: 0.3628 - val_loss: 0.4237
Epoch 23/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3892 - val_loss: 0.3523
Epoch 24/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3676 - val_loss: 0.3842
Epoch 25/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.4162
Epoch 26/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.3980
Epoch 27/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.3474
Epoch 28/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3725 - val_loss: 0.3920
Epoch 29/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3660 - val_loss: 0.3566
Epoch 30/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3700 - val_loss: 0.4191
Epoch 31/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3635 - val_loss: 0.3721
Epoch 32/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3628 - val_loss: 0.3948
Epoch 33/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3647 - val_loss: 0.3423
Epoch 34/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3547 - val_loss: 0.3453
Epoch 35/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3496 - val_loss: 0.4068
Epoch 36/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3476 - val_loss: 0.3417
Epoch 37/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3786 - val_loss: 0.3787
Epoch 38/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3379
Epoch 39/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3419
Epoch 40/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3522 - val_loss: 0.3705
Epoch 41/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3705 - val_loss: 0.3659
Epoch 42/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3545 - val_loss: 0.3803
Epoch 43/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3597 - val_loss: 0.3765
Epoch 44/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3443 - val_loss: 0.3814
Epoch 45/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.3326
Epoch 46/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.3385
Epoch 47/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3663 - val_loss: 0.3655
Epoch 48/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3479 - val_loss: 0.3579
Epoch 49/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3601 - val_loss: 0.3360
Epoch 50/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3616 - val_loss: 0.3318
Epoch 51/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3532 - val_loss: 0.3562
Epoch 52/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3427 - val_loss: 0.3520
Epoch 53/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3503 - val_loss: 0.4579
Epoch 54/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3402 - val_loss: 0.3808
Epoch 55/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3496 - val_loss: 0.3539
Epoch 56/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3401 - val_loss: 0.3723
Epoch 57/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3440 - val_loss: 0.3336
Epoch 58/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3348 - val_loss: 0.4011
Epoch 59/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3445 - val_loss: 0.3264
Epoch 60/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3414 - val_loss: 0.3271
Epoch 61/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3621 - val_loss: 0.3346
Epoch 62/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3497 - val_loss: 0.3493
Epoch 63/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3402
Epoch 64/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3299 - val_loss: 0.3275
Epoch 65/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3410 - val_loss: 0.3296
Epoch 66/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3364 - val_loss: 0.3307
Epoch 67/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3558 - val_loss: 0.3252
Epoch 68/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3372 - val_loss: 0.3242
Epoch 69/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3394 - val_loss: 0.3254
Epoch 70/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.3672
Epoch 71/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3428 - val_loss: 0.3375
Epoch 72/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3261 - val_loss: 0.3271
Epoch 73/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3409 - val_loss: 0.3242
Epoch 74/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3394 - val_loss: 0.3665
Epoch 75/100
363/363 [==============================] - 1s 4ms/step - loss: 0.3286 - val_loss: 0.3283
Epoch 76/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3391 - val_loss: 0.3240
Epoch 77/100
363/363 [==============================] - 1s 3ms/step - loss: 0.3293 - val_loss: 0.3381
Epoch 78/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3372 - val_loss: 0.3356
Epoch 79/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3364 - val_loss: 0.3224
Epoch 80/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3374 - val_loss: 0.3595
Epoch 81/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3381 - val_loss: 0.3432
Epoch 82/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3481 - val_loss: 0.3211
Epoch 83/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3441 - val_loss: 0.3342
Epoch 84/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3240 - val_loss: 0.4136
Epoch 85/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3303 - val_loss: 0.3285
Epoch 86/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3263 - val_loss: 0.3440
Epoch 87/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3733
Epoch 88/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3305 - val_loss: 0.3188
Epoch 89/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3283 - val_loss: 0.3492
Epoch 90/100
363/363 [==============================] - 1s 2ms/step - loss: 0.3243 - val_loss: 0.3175
Epoch 91/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3288 - val_loss: 0.3594
Epoch 92/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3343 - val_loss: 0.3169
Epoch 93/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3607
Epoch 94/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3262 - val_loss: 0.5184
Epoch 95/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3284 - val_loss: 0.7536
Epoch 96/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.5075
Epoch 97/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3290 - val_loss: 0.8087
Epoch 98/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3277 - val_loss: 1.0447
Epoch 99/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 1.6881
Epoch 100/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3706 - val_loss: 1.9265
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow.python.keras.callbacks.History at 0x1430c5c10&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mse_test</span> <span class="o">=</span> <span class="n">keras_reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>162/162 [==============================] - 0s 1ms/step - loss: 0.3409
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">keras_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:8 out of the last 10 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x140ae6040&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">reciprocal</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_distribs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;n_hidden&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="s2">&quot;n_neurons&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">reciprocal</span><span class="p">(</span><span class="mf">3e-4</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
<span class="p">}</span>

<span class="n">rnd_search_cv</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">keras_reg</span><span class="p">,</span>
                                   <span class="n">param_distribs</span><span class="p">,</span>
                                   <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                   <span class="n">cv</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                   <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">rnd_search_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span>
                  <span class="n">y_train</span><span class="p">,</span>
                  <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                  <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
                  <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting 3 folds for each of 10 candidates, totalling 30 fits
Epoch 1/100
242/242 [==============================] - 1s 3ms/step - loss: 1.3827 - val_loss: 0.4703
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4880 - val_loss: 0.4247
Epoch 3/100
242/242 [==============================] - 1s 4ms/step - loss: 0.4541 - val_loss: 0.4052
Epoch 4/100
242/242 [==============================] - 1s 5ms/step - loss: 0.4518 - val_loss: 0.3975
Epoch 5/100
242/242 [==============================] - 1s 4ms/step - loss: 0.4337 - val_loss: 0.3991
Epoch 6/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4263 - val_loss: 0.4031
Epoch 7/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4385 - val_loss: 0.4043
Epoch 8/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4301 - val_loss: 0.3929
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.4040
Epoch 10/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4200 - val_loss: 0.3886
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4099 - val_loss: 0.3999
Epoch 12/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3897 - val_loss: 0.4085
Epoch 13/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4265 - val_loss: 0.3922
Epoch 14/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.3918
Epoch 15/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4070 - val_loss: 0.3886
Epoch 16/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4032 - val_loss: 0.3933
Epoch 17/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4212 - val_loss: 0.3907
Epoch 18/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4241 - val_loss: 0.3955
Epoch 19/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4040 - val_loss: 0.3935
Epoch 20/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4146 - val_loss: 0.3891
121/121 [==============================] - 0s 1ms/step - loss: 0.4251
[CV] END learning_rate=0.022174573948353458, n_hidden=1, n_neurons=4; total time=  11.0s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 1.3852 - val_loss: 0.4860
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4722 - val_loss: 0.4280
Epoch 3/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4384 - val_loss: 0.5791
Epoch 4/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4422 - val_loss: 0.4549
Epoch 5/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4527 - val_loss: 0.5250
Epoch 6/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4474 - val_loss: 0.5486
Epoch 7/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4246 - val_loss: 0.5871
Epoch 8/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4382 - val_loss: 0.4759
Epoch 9/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4299 - val_loss: 0.7523
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4390 - val_loss: 0.7478
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4395 - val_loss: 0.8981
Epoch 12/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4234 - val_loss: 0.8543
121/121 [==============================] - 0s 2ms/step - loss: 0.4537
[CV] END learning_rate=0.022174573948353458, n_hidden=1, n_neurons=4; total time=   6.2s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 13.5523 - val_loss: 4.2468
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 1.2460 - val_loss: 0.5794
Epoch 3/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5520 - val_loss: 0.4357
Epoch 4/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4507 - val_loss: 0.4169
Epoch 5/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4365 - val_loss: 0.4135
Epoch 6/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4283 - val_loss: 0.4206
Epoch 7/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4420 - val_loss: 0.4100
Epoch 8/100
242/242 [==============================] - 1s 4ms/step - loss: 0.4636 - val_loss: 0.4155
Epoch 9/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4411 - val_loss: 0.4111
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4840 - val_loss: 0.4076
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4626 - val_loss: 0.4062
Epoch 12/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4326 - val_loss: 0.4078
Epoch 13/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4226 - val_loss: 0.4160
Epoch 14/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4260 - val_loss: 0.4158
Epoch 15/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4551 - val_loss: 0.4137
Epoch 16/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4552 - val_loss: 0.4069
Epoch 17/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4343 - val_loss: 0.4119
Epoch 18/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4453 - val_loss: 0.4149
Epoch 19/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4530 - val_loss: 0.4081
Epoch 20/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4502 - val_loss: 0.4141
Epoch 21/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4420 - val_loss: 0.4100
121/121 [==============================] - 0s 1ms/step - loss: 0.4473
[CV] END learning_rate=0.022174573948353458, n_hidden=1, n_neurons=4; total time=  12.3s
Epoch 1/100
242/242 [==============================] - 1s 3ms/step - loss: 1.7737 - val_loss: 6.2480
Epoch 2/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5899 - val_loss: 5.2166
Epoch 3/100
242/242 [==============================] - 1s 2ms/step - loss: 0.5147 - val_loss: 0.4474
Epoch 4/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4477 - val_loss: 0.3901
Epoch 5/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3980 - val_loss: 0.3736
Epoch 6/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3757 - val_loss: 0.3803
Epoch 7/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3701 - val_loss: 0.3813
Epoch 8/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3632 - val_loss: 0.3961
Epoch 9/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3512 - val_loss: 0.3988
Epoch 10/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3532 - val_loss: 0.3891
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3358 - val_loss: 0.3870
Epoch 12/100
242/242 [==============================] - 1s 6ms/step - loss: 0.3252 - val_loss: 0.3770
Epoch 13/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3499 - val_loss: 0.3770
Epoch 14/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3382 - val_loss: 0.3843
Epoch 15/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3331 - val_loss: 0.3770
121/121 [==============================] - 0s 1ms/step - loss: 0.3561
[CV] END learning_rate=0.005432590230265343, n_hidden=2, n_neurons=94; total time=  12.0s
Epoch 1/100
242/242 [==============================] - 1s 4ms/step - loss: 1.5396 - val_loss: 3.5738
Epoch 2/100
242/242 [==============================] - 1s 2ms/step - loss: 0.5129 - val_loss: 0.7767
Epoch 3/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4283 - val_loss: 0.5515
Epoch 4/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4097 - val_loss: 0.5335
Epoch 5/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3926 - val_loss: 0.5336
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3760 - val_loss: 0.6750
Epoch 7/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3580 - val_loss: 0.8462
Epoch 8/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3622 - val_loss: 0.8724
Epoch 9/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3518 - val_loss: 0.9645
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3560 - val_loss: 0.7225
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3517 - val_loss: 0.7257
Epoch 12/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3367 - val_loss: 0.7218
Epoch 13/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3334 - val_loss: 0.8428
Epoch 14/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3244 - val_loss: 0.7061
121/121 [==============================] - 0s 789us/step - loss: 0.3650
[CV] END learning_rate=0.005432590230265343, n_hidden=2, n_neurons=94; total time=   7.9s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 1.7832 - val_loss: 2.9433
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5594 - val_loss: 4.2557
Epoch 3/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4797 - val_loss: 2.8526
Epoch 4/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4774 - val_loss: 1.6798
Epoch 5/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.4322
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3736 - val_loss: 0.4172
Epoch 7/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3719 - val_loss: 0.3769
Epoch 8/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3883 - val_loss: 0.3688
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 0.4032
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3587 - val_loss: 0.3418
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.4452
Epoch 12/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3454
Epoch 13/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3258 - val_loss: 0.3395
Epoch 14/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3368 - val_loss: 0.4354
Epoch 15/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3603 - val_loss: 0.3386
Epoch 16/100
242/242 [==============================] - 1s 5ms/step - loss: 0.3421 - val_loss: 0.4038
Epoch 17/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3297 - val_loss: 0.3302
Epoch 18/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3386 - val_loss: 0.3580
Epoch 19/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3523 - val_loss: 0.3545
Epoch 20/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3401 - val_loss: 0.3459
Epoch 21/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3327 - val_loss: 0.3245
Epoch 22/100
242/242 [==============================] - 1s 5ms/step - loss: 0.3455 - val_loss: 0.3256
Epoch 23/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3303 - val_loss: 0.3435
Epoch 24/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3216 - val_loss: 0.3385
Epoch 25/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3195 - val_loss: 0.3660
Epoch 26/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3209 - val_loss: 0.3963
Epoch 27/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3396 - val_loss: 0.3146
Epoch 28/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3305 - val_loss: 0.3194
Epoch 29/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3264 - val_loss: 0.4215
Epoch 30/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3204 - val_loss: 0.3238
Epoch 31/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3094 - val_loss: 0.3141
Epoch 32/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3294 - val_loss: 0.4323
Epoch 33/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3191 - val_loss: 0.3116
Epoch 34/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3035 - val_loss: 0.4257
Epoch 35/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3127 - val_loss: 0.4807
Epoch 36/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3009 - val_loss: 0.6419
Epoch 37/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3118 - val_loss: 0.6205
Epoch 38/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3294 - val_loss: 1.0407
Epoch 39/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3227 - val_loss: 0.5648
Epoch 40/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3144 - val_loss: 0.9436
Epoch 41/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3156 - val_loss: 0.3885
Epoch 42/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3000 - val_loss: 0.7540
Epoch 43/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3166 - val_loss: 1.0124
121/121 [==============================] - 0s 998us/step - loss: 0.3183
[CV] END learning_rate=0.005432590230265343, n_hidden=2, n_neurons=94; total time=  27.5s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 5.2328 - val_loss: 13.3699
Epoch 2/100
242/242 [==============================] - 1s 3ms/step - loss: 2.4156 - val_loss: 10.8972
Epoch 3/100
242/242 [==============================] - 1s 4ms/step - loss: 1.4953 - val_loss: 7.7330
Epoch 4/100
242/242 [==============================] - 1s 3ms/step - loss: 1.1092 - val_loss: 5.0744
Epoch 5/100
242/242 [==============================] - 1s 2ms/step - loss: 0.8935 - val_loss: 3.2363
Epoch 6/100
242/242 [==============================] - 1s 2ms/step - loss: 0.8194 - val_loss: 2.1597
Epoch 7/100
242/242 [==============================] - 0s 1ms/step - loss: 0.7802 - val_loss: 1.4840
Epoch 8/100
242/242 [==============================] - 0s 2ms/step - loss: 0.7285 - val_loss: 1.1083
Epoch 9/100
242/242 [==============================] - 1s 3ms/step - loss: 0.6921 - val_loss: 0.8942
Epoch 10/100
242/242 [==============================] - 1s 3ms/step - loss: 0.6951 - val_loss: 0.7687
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6599 - val_loss: 0.6947
Epoch 12/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6237 - val_loss: 0.6524
Epoch 13/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6619 - val_loss: 0.6234
Epoch 14/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6487 - val_loss: 0.6061
Epoch 15/100
242/242 [==============================] - 1s 3ms/step - loss: 0.6429 - val_loss: 0.5933
Epoch 16/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6103 - val_loss: 0.5819
Epoch 17/100
242/242 [==============================] - 1s 3ms/step - loss: 0.6492 - val_loss: 0.5733
Epoch 18/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6227 - val_loss: 0.5650
Epoch 19/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6024 - val_loss: 0.5578
Epoch 20/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5992 - val_loss: 0.5508
Epoch 21/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5874 - val_loss: 0.5446
Epoch 22/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5653 - val_loss: 0.5384
Epoch 23/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.5326
Epoch 24/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5639 - val_loss: 0.5266
Epoch 25/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5689 - val_loss: 0.5214
Epoch 26/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5438 - val_loss: 0.5166
Epoch 27/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5653 - val_loss: 0.5116
Epoch 28/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5540 - val_loss: 0.5076
Epoch 29/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5635 - val_loss: 0.5035
Epoch 30/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5438 - val_loss: 0.4989
Epoch 31/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5203 - val_loss: 0.4946
Epoch 32/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 0.4915
Epoch 33/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5159 - val_loss: 0.4883
Epoch 34/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5200 - val_loss: 0.4856
Epoch 35/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5080 - val_loss: 0.4828
Epoch 36/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4939 - val_loss: 0.4789
Epoch 37/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5136 - val_loss: 0.4780
Epoch 38/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4794 - val_loss: 0.4742
Epoch 39/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5037 - val_loss: 0.4729
Epoch 40/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4797 - val_loss: 0.4714
Epoch 41/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4870 - val_loss: 0.4686
Epoch 42/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5008 - val_loss: 0.4666
Epoch 43/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4892 - val_loss: 0.4646
Epoch 44/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4786 - val_loss: 0.4636
Epoch 45/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4812 - val_loss: 0.4616
Epoch 46/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4710 - val_loss: 0.4582
Epoch 47/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4787 - val_loss: 0.4581
Epoch 48/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4574 - val_loss: 0.4573
Epoch 49/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4752 - val_loss: 0.4560
Epoch 50/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4641 - val_loss: 0.4544
Epoch 51/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4536 - val_loss: 0.4525
Epoch 52/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4800 - val_loss: 0.4527
Epoch 53/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4622 - val_loss: 0.4522
Epoch 54/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4648 - val_loss: 0.4509
Epoch 55/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4387 - val_loss: 0.4509
Epoch 56/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4897 - val_loss: 0.4513
Epoch 57/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4494 - val_loss: 0.4496
Epoch 58/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4645 - val_loss: 0.4510
Epoch 59/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4580 - val_loss: 0.4502
Epoch 60/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4393 - val_loss: 0.4478
Epoch 61/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4271 - val_loss: 0.4485
Epoch 62/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4362 - val_loss: 0.4488
Epoch 63/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4499 - val_loss: 0.4477
Epoch 64/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4377 - val_loss: 0.4497
Epoch 65/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4153 - val_loss: 0.4512
Epoch 66/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4718 - val_loss: 0.4484
Epoch 67/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4375 - val_loss: 0.4483
Epoch 68/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4494
Epoch 69/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4460 - val_loss: 0.4492
Epoch 70/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4546 - val_loss: 0.4476
Epoch 71/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4346 - val_loss: 0.4481
Epoch 72/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4486 - val_loss: 0.4503
Epoch 73/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4463 - val_loss: 0.4486
Epoch 74/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4312 - val_loss: 0.4491
Epoch 75/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4328 - val_loss: 0.4496
Epoch 76/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4235 - val_loss: 0.4483
Epoch 77/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.4474
Epoch 78/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4187 - val_loss: 0.4490
Epoch 79/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4158 - val_loss: 0.4495
Epoch 80/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4270 - val_loss: 0.4468
Epoch 81/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3977 - val_loss: 0.4492
Epoch 82/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4071 - val_loss: 0.4525
Epoch 83/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3911 - val_loss: 0.4504
Epoch 84/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4320 - val_loss: 0.4525
Epoch 85/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4206 - val_loss: 0.4495
Epoch 86/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3961 - val_loss: 0.4548
Epoch 87/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4143 - val_loss: 0.4512
Epoch 88/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4150 - val_loss: 0.4481
Epoch 89/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4045 - val_loss: 0.4472
Epoch 90/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3976 - val_loss: 0.4506
121/121 [==============================] - 0s 1ms/step - loss: 0.4209
[CV] END learning_rate=0.00037078874137762145, n_hidden=1, n_neurons=51; total time=  38.2s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 4.4546 - val_loss: 7.5238
Epoch 2/100
242/242 [==============================] - 1s 4ms/step - loss: 1.7950 - val_loss: 8.6120
Epoch 3/100
242/242 [==============================] - 1s 4ms/step - loss: 1.1115 - val_loss: 8.4896
Epoch 4/100
242/242 [==============================] - 1s 3ms/step - loss: 0.9287 - val_loss: 7.7423
Epoch 5/100
242/242 [==============================] - 1s 2ms/step - loss: 0.8253 - val_loss: 6.8202
Epoch 6/100
242/242 [==============================] - 1s 2ms/step - loss: 0.7837 - val_loss: 5.9344
Epoch 7/100
242/242 [==============================] - 1s 2ms/step - loss: 0.7711 - val_loss: 5.1492
Epoch 8/100
242/242 [==============================] - 0s 2ms/step - loss: 0.7292 - val_loss: 4.4548
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.7279 - val_loss: 3.9122
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 0.7055 - val_loss: 3.4233
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6956 - val_loss: 2.9997
Epoch 12/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6640 - val_loss: 2.6082
Epoch 13/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6539 - val_loss: 2.2766
Epoch 14/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6447 - val_loss: 1.9984
Epoch 15/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6831 - val_loss: 1.7447
Epoch 16/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6364 - val_loss: 1.5300
Epoch 17/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6425 - val_loss: 1.3410
Epoch 18/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6340 - val_loss: 1.1762
Epoch 19/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6140 - val_loss: 1.0345
Epoch 20/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6101 - val_loss: 0.9174
Epoch 21/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5938 - val_loss: 0.8153
Epoch 22/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5975 - val_loss: 0.7363
Epoch 23/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5956 - val_loss: 0.6696
Epoch 24/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5714 - val_loss: 0.6187
Epoch 25/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5535 - val_loss: 0.5778
Epoch 26/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5553 - val_loss: 0.5491
Epoch 27/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5725 - val_loss: 0.5299
Epoch 28/100
242/242 [==============================] - 1s 2ms/step - loss: 0.5688 - val_loss: 0.5199
Epoch 29/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5619 - val_loss: 0.5172
Epoch 30/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5510 - val_loss: 0.5206
Epoch 31/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5401 - val_loss: 0.5312
Epoch 32/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5370 - val_loss: 0.5447
Epoch 33/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5423 - val_loss: 0.5639
Epoch 34/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5211 - val_loss: 0.5821
Epoch 35/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5081 - val_loss: 0.6039
Epoch 36/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5036 - val_loss: 0.6306
Epoch 37/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5191 - val_loss: 0.6564
Epoch 38/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4928 - val_loss: 0.6820
Epoch 39/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5096 - val_loss: 0.7087
121/121 [==============================] - 0s 2ms/step - loss: 0.5160
[CV] END learning_rate=0.00037078874137762145, n_hidden=1, n_neurons=51; total time=  19.1s
Epoch 1/100
242/242 [==============================] - 1s 3ms/step - loss: 4.8993 - val_loss: 7.4460
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 2.4173 - val_loss: 5.2071
Epoch 3/100
242/242 [==============================] - 0s 1ms/step - loss: 1.4701 - val_loss: 2.9554
Epoch 4/100
242/242 [==============================] - 0s 1ms/step - loss: 1.1716 - val_loss: 1.7752
Epoch 5/100
242/242 [==============================] - 0s 2ms/step - loss: 0.9242 - val_loss: 1.1201
Epoch 6/100
242/242 [==============================] - 0s 1ms/step - loss: 0.8307 - val_loss: 0.8519
Epoch 7/100
242/242 [==============================] - 1s 3ms/step - loss: 0.7780 - val_loss: 0.7512
Epoch 8/100
242/242 [==============================] - 0s 2ms/step - loss: 0.7632 - val_loss: 0.7064
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.7361 - val_loss: 0.6896
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6859 - val_loss: 0.6760
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.7064 - val_loss: 0.6687
Epoch 12/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6826 - val_loss: 0.6577
Epoch 13/100
242/242 [==============================] - 1s 2ms/step - loss: 0.6416 - val_loss: 0.6454
Epoch 14/100
242/242 [==============================] - 1s 5ms/step - loss: 0.6539 - val_loss: 0.6355
Epoch 15/100
242/242 [==============================] - 1s 4ms/step - loss: 0.6736 - val_loss: 0.6256
Epoch 16/100
242/242 [==============================] - 1s 5ms/step - loss: 0.6529 - val_loss: 0.6213
Epoch 17/100
242/242 [==============================] - 1s 2ms/step - loss: 0.6253 - val_loss: 0.6120
Epoch 18/100
242/242 [==============================] - 1s 3ms/step - loss: 0.6484 - val_loss: 0.6024
Epoch 19/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6228 - val_loss: 0.5998
Epoch 20/100
242/242 [==============================] - 1s 3ms/step - loss: 0.6079 - val_loss: 0.5901
Epoch 21/100
242/242 [==============================] - 1s 2ms/step - loss: 0.5912 - val_loss: 0.5822
Epoch 22/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6250 - val_loss: 0.5763
Epoch 23/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5804 - val_loss: 0.5664
Epoch 24/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5735 - val_loss: 0.5574
Epoch 25/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5793 - val_loss: 0.5527
Epoch 26/100
242/242 [==============================] - 1s 4ms/step - loss: 0.5576 - val_loss: 0.5452
Epoch 27/100
242/242 [==============================] - 1s 4ms/step - loss: 0.5786 - val_loss: 0.5437
Epoch 28/100
242/242 [==============================] - 1s 2ms/step - loss: 0.5683 - val_loss: 0.5366
Epoch 29/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5475 - val_loss: 0.5322
Epoch 30/100
242/242 [==============================] - 1s 2ms/step - loss: 0.5471 - val_loss: 0.5264
Epoch 31/100
242/242 [==============================] - 1s 4ms/step - loss: 0.5281 - val_loss: 0.5234
Epoch 32/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5547 - val_loss: 0.5175
Epoch 33/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5437 - val_loss: 0.5137
Epoch 34/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5113 - val_loss: 0.5078
Epoch 35/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5125 - val_loss: 0.5045
Epoch 36/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5127 - val_loss: 0.4970
Epoch 37/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5158 - val_loss: 0.4911
Epoch 38/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5132 - val_loss: 0.4887
Epoch 39/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5140 - val_loss: 0.4847
Epoch 40/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4995 - val_loss: 0.4815
Epoch 41/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4795 - val_loss: 0.4776
Epoch 42/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4838 - val_loss: 0.4736
Epoch 43/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4966 - val_loss: 0.4706
Epoch 44/100
242/242 [==============================] - 1s 4ms/step - loss: 0.4932 - val_loss: 0.4673
Epoch 45/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4810 - val_loss: 0.4655
Epoch 46/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4737 - val_loss: 0.4625
Epoch 47/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4866 - val_loss: 0.4576
Epoch 48/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4719 - val_loss: 0.4554
Epoch 49/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4859 - val_loss: 0.4525
Epoch 50/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4618 - val_loss: 0.4495
Epoch 51/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4862 - val_loss: 0.4468
Epoch 52/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4576 - val_loss: 0.4446
Epoch 53/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4641 - val_loss: 0.4420
Epoch 54/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4641 - val_loss: 0.4394
Epoch 55/100
242/242 [==============================] - 1s 4ms/step - loss: 0.4560 - val_loss: 0.4373
Epoch 56/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4620 - val_loss: 0.4349
Epoch 57/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4542 - val_loss: 0.4330
Epoch 58/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4659 - val_loss: 0.4311
Epoch 59/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4584 - val_loss: 0.4291
Epoch 60/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4684 - val_loss: 0.4277
Epoch 61/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4678 - val_loss: 0.4257
Epoch 62/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4516 - val_loss: 0.4241
Epoch 63/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4410 - val_loss: 0.4224
Epoch 64/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4622 - val_loss: 0.4208
Epoch 65/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4594 - val_loss: 0.4193
Epoch 66/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4354 - val_loss: 0.4180
Epoch 67/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4460 - val_loss: 0.4164
Epoch 68/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4504 - val_loss: 0.4151
Epoch 69/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4247 - val_loss: 0.4141
Epoch 70/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4433 - val_loss: 0.4124
Epoch 71/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4618 - val_loss: 0.4112
Epoch 72/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4168 - val_loss: 0.4101
Epoch 73/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4359 - val_loss: 0.4088
Epoch 74/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4307 - val_loss: 0.4081
Epoch 75/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4273 - val_loss: 0.4073
Epoch 76/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4386 - val_loss: 0.4070
Epoch 77/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4324 - val_loss: 0.4056
Epoch 78/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4313 - val_loss: 0.4040
Epoch 79/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4328 - val_loss: 0.4034
Epoch 80/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4146 - val_loss: 0.4033
Epoch 81/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4138 - val_loss: 0.4019
Epoch 82/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4241 - val_loss: 0.4008
Epoch 83/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4112 - val_loss: 0.4002
Epoch 84/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4320 - val_loss: 0.3996
Epoch 85/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4259 - val_loss: 0.3983
Epoch 86/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4110 - val_loss: 0.3980
Epoch 87/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4237 - val_loss: 0.3981
Epoch 88/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4058 - val_loss: 0.3969
Epoch 89/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4303 - val_loss: 0.3978
Epoch 90/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4157 - val_loss: 0.3961
Epoch 91/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4229 - val_loss: 0.3951
Epoch 92/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4118 - val_loss: 0.3938
Epoch 93/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4005 - val_loss: 0.3938
Epoch 94/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4222 - val_loss: 0.3935
Epoch 95/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4279 - val_loss: 0.3934
Epoch 96/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4027 - val_loss: 0.3932
Epoch 97/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4108 - val_loss: 0.3939
Epoch 98/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3998 - val_loss: 0.3913
Epoch 99/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4002 - val_loss: 0.3916
Epoch 100/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4052 - val_loss: 0.3918
121/121 [==============================] - 0s 3ms/step - loss: 0.4139
[CV] END learning_rate=0.00037078874137762145, n_hidden=1, n_neurons=51; total time=  58.6s
Epoch 1/100
242/242 [==============================] - 1s 4ms/step - loss: 3.4453 - val_loss: 1.3536
Epoch 2/100
242/242 [==============================] - 1s 2ms/step - loss: 0.7645 - val_loss: 0.7463
Epoch 3/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6590 - val_loss: 0.5899
Epoch 4/100
242/242 [==============================] - 1s 3ms/step - loss: 0.6064 - val_loss: 0.5366
Epoch 5/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5580 - val_loss: 0.5063
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5150 - val_loss: 0.4813
Epoch 7/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4985 - val_loss: 0.4639
Epoch 8/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4716 - val_loss: 0.4427
Epoch 9/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4477 - val_loss: 0.4393
Epoch 10/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4411 - val_loss: 0.4137
Epoch 11/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4204 - val_loss: 0.4071
Epoch 12/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3978 - val_loss: 0.3983
Epoch 13/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4290 - val_loss: 0.3933
Epoch 14/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4111 - val_loss: 0.3972
Epoch 15/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.3852
Epoch 16/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3923 - val_loss: 0.3830
Epoch 17/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4072 - val_loss: 0.3947
Epoch 18/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4075 - val_loss: 0.3713
Epoch 19/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3829 - val_loss: 0.3752
Epoch 20/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3867 - val_loss: 0.3741
Epoch 21/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3800 - val_loss: 0.3782
Epoch 22/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3686 - val_loss: 0.3637
Epoch 23/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3795 - val_loss: 0.3723
Epoch 24/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3691 - val_loss: 0.3707
Epoch 25/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3751 - val_loss: 0.4047
Epoch 26/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3599 - val_loss: 0.3839
Epoch 27/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3795 - val_loss: 0.4167
Epoch 28/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3664 - val_loss: 0.3500
Epoch 29/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3666 - val_loss: 0.3792
Epoch 30/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3629 - val_loss: 0.3636
Epoch 31/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3586 - val_loss: 0.3476
Epoch 32/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3489 - val_loss: 0.3566
Epoch 33/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3517 - val_loss: 0.3611
Epoch 34/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3521 - val_loss: 0.3414
Epoch 35/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3499 - val_loss: 0.3474
Epoch 36/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3469 - val_loss: 0.3944
Epoch 37/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3453 - val_loss: 0.4403
Epoch 38/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.4722
Epoch 39/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3449 - val_loss: 0.3722
Epoch 40/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3307 - val_loss: 0.4019
Epoch 41/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3408 - val_loss: 0.3376
Epoch 42/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3456 - val_loss: 0.3377
Epoch 43/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3436 - val_loss: 0.3354
Epoch 44/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3430 - val_loss: 0.3738
Epoch 45/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3496 - val_loss: 0.3336
Epoch 46/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3306 - val_loss: 0.3563
Epoch 47/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3370 - val_loss: 0.3547
Epoch 48/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3262 - val_loss: 0.3398
Epoch 49/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3510 - val_loss: 0.3304
Epoch 50/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.3850
Epoch 51/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3238 - val_loss: 0.3430
Epoch 52/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3495 - val_loss: 0.3363
Epoch 53/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3329 - val_loss: 0.3387
Epoch 54/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3366 - val_loss: 0.3294
Epoch 55/100
242/242 [==============================] - 1s 5ms/step - loss: 0.3260 - val_loss: 0.3654
Epoch 56/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3558 - val_loss: 0.3310
Epoch 57/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3366 - val_loss: 0.3730
Epoch 58/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3374 - val_loss: 0.3374
Epoch 59/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3415 - val_loss: 0.3263
Epoch 60/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3332 - val_loss: 0.3402
Epoch 61/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3199 - val_loss: 0.3440
Epoch 62/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3275 - val_loss: 0.3581
Epoch 63/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3341 - val_loss: 0.3303
Epoch 64/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3222 - val_loss: 0.3680
Epoch 65/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3119 - val_loss: 0.3292
Epoch 66/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3518 - val_loss: 0.3276
Epoch 67/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3314 - val_loss: 0.3562
Epoch 68/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3287 - val_loss: 0.3295
Epoch 69/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3316 - val_loss: 0.3439
121/121 [==============================] - 0s 1ms/step - loss: 0.3550
[CV] END learning_rate=0.0016535051383872363, n_hidden=2, n_neurons=70; total time=  41.3s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 3.2276 - val_loss: 3.4090
Epoch 2/100
242/242 [==============================] - 0s 1ms/step - loss: 0.7673 - val_loss: 1.6754
Epoch 3/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6369 - val_loss: 0.9319
Epoch 4/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6031 - val_loss: 0.6042
Epoch 5/100
242/242 [==============================] - 1s 2ms/step - loss: 0.5652 - val_loss: 0.5061
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 0.5058
Epoch 7/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4977 - val_loss: 0.5272
Epoch 8/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4759 - val_loss: 0.5600
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4640 - val_loss: 0.5367
Epoch 10/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4464 - val_loss: 0.5221
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4347 - val_loss: 0.4878
Epoch 12/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4162 - val_loss: 0.4531
Epoch 13/100
242/242 [==============================] - 1s 2ms/step - loss: 0.4113 - val_loss: 0.4182
Epoch 14/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3961 - val_loss: 0.3877
Epoch 15/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4222 - val_loss: 0.3818
Epoch 16/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3975 - val_loss: 0.4022
Epoch 17/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3999 - val_loss: 0.4348
Epoch 18/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4069 - val_loss: 0.4935
Epoch 19/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3895 - val_loss: 0.5340
Epoch 20/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3871 - val_loss: 0.5982
Epoch 21/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3838 - val_loss: 0.6541
Epoch 22/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3802 - val_loss: 0.7245
Epoch 23/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3840 - val_loss: 0.8045
Epoch 24/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3677 - val_loss: 0.8587
Epoch 25/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.9089
121/121 [==============================] - 0s 766us/step - loss: 0.3884
[CV] END learning_rate=0.0016535051383872363, n_hidden=2, n_neurons=70; total time=  11.7s
Epoch 1/100
242/242 [==============================] - 1s 3ms/step - loss: 3.3058 - val_loss: 2.1643
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 0.7651 - val_loss: 0.6141
Epoch 3/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6136 - val_loss: 0.5601
Epoch 4/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5596 - val_loss: 0.5241
Epoch 5/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5239 - val_loss: 0.5017
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4854 - val_loss: 0.4749
Epoch 7/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4781 - val_loss: 0.4558
Epoch 8/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4755 - val_loss: 0.4297
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4463 - val_loss: 0.4464
Epoch 10/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4310 - val_loss: 0.4189
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4329 - val_loss: 0.4438
Epoch 12/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.4250
Epoch 13/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3859 - val_loss: 0.4009
Epoch 14/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3923 - val_loss: 0.4403
Epoch 15/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4130 - val_loss: 0.4014
Epoch 16/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3992 - val_loss: 0.4247
Epoch 17/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3806 - val_loss: 0.3964
Epoch 18/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3962 - val_loss: 0.3974
Epoch 19/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3999 - val_loss: 0.4229
Epoch 20/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3901 - val_loss: 0.4053
Epoch 21/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3831 - val_loss: 0.3989
Epoch 22/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3974 - val_loss: 0.3957
Epoch 23/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3763 - val_loss: 0.3864
Epoch 24/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3718 - val_loss: 0.4022
Epoch 25/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3711 - val_loss: 0.3729
Epoch 26/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3712 - val_loss: 0.3645
Epoch 27/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3836 - val_loss: 0.4107
Epoch 28/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3786 - val_loss: 0.3925
Epoch 29/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3648 - val_loss: 0.4265
Epoch 30/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3695 - val_loss: 0.3879
Epoch 31/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3592 - val_loss: 0.3789
Epoch 32/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3819 - val_loss: 0.4080
Epoch 33/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3659 - val_loss: 0.3873
Epoch 34/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3501 - val_loss: 0.4232
Epoch 35/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3550 - val_loss: 0.3718
Epoch 36/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3490 - val_loss: 0.3663
121/121 [==============================] - 0s 1ms/step - loss: 0.3555
[CV] END learning_rate=0.0016535051383872363, n_hidden=2, n_neurons=70; total time=  15.4s
Epoch 1/100
242/242 [==============================] - 1s 3ms/step - loss: 1.9995 - val_loss: 297.3653
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 1.2481 - val_loss: 539.0366
Epoch 3/100
242/242 [==============================] - 0s 1ms/step - loss: 2.5441 - val_loss: 3736.4512
Epoch 4/100
242/242 [==============================] - 0s 1ms/step - loss: 7.4651 - val_loss: 12227.6963
Epoch 5/100
242/242 [==============================] - 0s 1ms/step - loss: 22.4715 - val_loss: 61529.0664
Epoch 6/100
242/242 [==============================] - 0s 1ms/step - loss: 2537.2815 - val_loss: 268363.3750
Epoch 7/100
242/242 [==============================] - 0s 1ms/step - loss: 2132.8528 - val_loss: 1210516.8750
Epoch 8/100
242/242 [==============================] - 0s 1ms/step - loss: 32696.8221 - val_loss: 5411000.5000
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 28036.2842 - val_loss: 24506666.0000
Epoch 10/100
242/242 [==============================] - 0s 1ms/step - loss: 910065.1961 - val_loss: 119812880.0000
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 1721895.0969 - val_loss: 529730336.0000
121/121 [==============================] - 0s 738us/step - loss: 1402363.6250
[CV] END learning_rate=0.01824796188192035, n_hidden=0, n_neurons=40; total time=   4.4s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 2.2323 - val_loss: 15.8284
Epoch 2/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5228 - val_loss: 22.4892
Epoch 3/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5292 - val_loss: 24.7894
Epoch 4/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5081 - val_loss: 22.4864
Epoch 5/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5095 - val_loss: 21.9009
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5093 - val_loss: 21.2895
Epoch 7/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4936 - val_loss: 19.9064
Epoch 8/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5221 - val_loss: 22.5013
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5027 - val_loss: 20.0987
Epoch 10/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4831 - val_loss: 10.7128
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5128 - val_loss: 19.7319
Epoch 12/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4957 - val_loss: 24.3237
Epoch 13/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5081 - val_loss: 25.9485
Epoch 14/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4740 - val_loss: 10.5277
Epoch 15/100
242/242 [==============================] - 1s 2ms/step - loss: 0.5379 - val_loss: 17.1916
Epoch 16/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5078 - val_loss: 21.8347
Epoch 17/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4993 - val_loss: 11.7743
Epoch 18/100
242/242 [==============================] - 1s 3ms/step - loss: 0.5211 - val_loss: 14.1555
Epoch 19/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5103 - val_loss: 20.9814
Epoch 20/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4812 - val_loss: 12.3621
Epoch 21/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5134 - val_loss: 25.9146
Epoch 22/100
242/242 [==============================] - 1s 4ms/step - loss: 0.4890 - val_loss: 16.0461
Epoch 23/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5148 - val_loss: 19.4877
Epoch 24/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4909 - val_loss: 12.1054
121/121 [==============================] - 0s 2ms/step - loss: 0.7813
[CV] END learning_rate=0.01824796188192035, n_hidden=0, n_neurons=40; total time=  11.7s
Epoch 1/100
242/242 [==============================] - 1s 3ms/step - loss: 1.9669 - val_loss: 307.7497
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 1.0908 - val_loss: 76.3015
Epoch 3/100
242/242 [==============================] - 1s 4ms/step - loss: 0.8437 - val_loss: 795.2294
Epoch 4/100
242/242 [==============================] - 1s 3ms/step - loss: 41.8219 - val_loss: 704.0453
Epoch 5/100
242/242 [==============================] - 1s 2ms/step - loss: 1.0379 - val_loss: 2668.0312
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 6.1716 - val_loss: 1446.2616
Epoch 7/100
242/242 [==============================] - 1s 3ms/step - loss: 3.3018 - val_loss: 1540.5388
Epoch 8/100
242/242 [==============================] - 0s 2ms/step - loss: 71.5701 - val_loss: 1396.7128
Epoch 9/100
242/242 [==============================] - 1s 3ms/step - loss: 6.0212 - val_loss: 1334.0870
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 2.0299 - val_loss: 216.7278
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 21.3466 - val_loss: 125.2071
Epoch 12/100
242/242 [==============================] - 1s 2ms/step - loss: 0.7510 - val_loss: 2.2903
Epoch 13/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4922 - val_loss: 790.5432
Epoch 14/100
242/242 [==============================] - 0s 1ms/step - loss: 5.4409 - val_loss: 468.7433
Epoch 15/100
242/242 [==============================] - 0s 1ms/step - loss: 1.0499 - val_loss: 1073.9164
Epoch 16/100
242/242 [==============================] - 0s 1ms/step - loss: 45.2525 - val_loss: 865.6384
Epoch 17/100
242/242 [==============================] - 0s 1ms/step - loss: 2.7759 - val_loss: 1128.1509
Epoch 18/100
242/242 [==============================] - 0s 1ms/step - loss: 2.7236 - val_loss: 499.5195
Epoch 19/100
242/242 [==============================] - 0s 1ms/step - loss: 34.6840 - val_loss: 309.7943
Epoch 20/100
242/242 [==============================] - 0s 1ms/step - loss: 2.3475 - val_loss: 354.6348
Epoch 21/100
242/242 [==============================] - 0s 1ms/step - loss: 2.4646 - val_loss: 559.4493
Epoch 22/100
242/242 [==============================] - 1s 2ms/step - loss: 3.0812 - val_loss: 393.8702
121/121 [==============================] - 0s 710us/step - loss: 0.6226
[CV] END learning_rate=0.01824796188192035, n_hidden=0, n_neurons=40; total time=  11.1s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 1.9862 - val_loss: 1.4543
Epoch 2/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6404 - val_loss: 0.9557
Epoch 3/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5532 - val_loss: 0.4628
Epoch 4/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4956 - val_loss: 0.4214
Epoch 5/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.3984
Epoch 6/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4032 - val_loss: 0.4056
Epoch 7/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4015 - val_loss: 0.3741
Epoch 8/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3915 - val_loss: 0.3926
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.3832
Epoch 10/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.3929
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.3570
Epoch 12/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3410 - val_loss: 0.3790
Epoch 13/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3720 - val_loss: 0.3840
Epoch 14/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3560 - val_loss: 0.3950
Epoch 15/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3490 - val_loss: 0.3751
Epoch 16/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3955
Epoch 17/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.3900
Epoch 18/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3621 - val_loss: 0.3902
Epoch 19/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3365 - val_loss: 0.3942
Epoch 20/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3438 - val_loss: 0.3811
Epoch 21/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3389 - val_loss: 0.3907
121/121 [==============================] - 0s 730us/step - loss: 0.3624
[CV] END learning_rate=0.0045455096956331, n_hidden=3, n_neurons=30; total time=   7.9s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 1.7235 - val_loss: 0.5822
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5852 - val_loss: 0.4873
Epoch 3/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4784 - val_loss: 0.4420
Epoch 4/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4436 - val_loss: 0.4139
Epoch 5/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4223 - val_loss: 0.4132
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3955 - val_loss: 0.4464
Epoch 7/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3768 - val_loss: 0.4717
Epoch 8/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3800 - val_loss: 0.5331
Epoch 9/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3620 - val_loss: 0.6951
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3645 - val_loss: 0.6944
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3646 - val_loss: 0.8506
Epoch 12/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3432 - val_loss: 0.7660
Epoch 13/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3419 - val_loss: 0.8731
Epoch 14/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3305 - val_loss: 0.9306
Epoch 15/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3605 - val_loss: 0.9345
121/121 [==============================] - 0s 774us/step - loss: 0.3685
[CV] END learning_rate=0.0045455096956331, n_hidden=3, n_neurons=30; total time=   6.2s
Epoch 1/100
242/242 [==============================] - 1s 3ms/step - loss: 1.7434 - val_loss: 0.6796
Epoch 2/100
242/242 [==============================] - 1s 2ms/step - loss: 0.5868 - val_loss: 0.4957
Epoch 3/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5017 - val_loss: 0.4633
Epoch 4/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4473 - val_loss: 0.4565
Epoch 5/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4218 - val_loss: 0.4150
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3928 - val_loss: 0.4331
Epoch 7/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3924 - val_loss: 0.3887
Epoch 8/100
242/242 [==============================] - 1s 5ms/step - loss: 0.4047 - val_loss: 0.3785
Epoch 9/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3805 - val_loss: 0.4233
Epoch 10/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3749 - val_loss: 0.3652
Epoch 11/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3866 - val_loss: 0.4336
Epoch 12/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3635 - val_loss: 0.3763
Epoch 13/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3414 - val_loss: 0.3632
Epoch 14/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3498 - val_loss: 0.4460
Epoch 15/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3760 - val_loss: 0.3555
Epoch 16/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3554 - val_loss: 0.3947
Epoch 17/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3443 - val_loss: 0.3623
Epoch 18/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3532 - val_loss: 0.3774
Epoch 19/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3664 - val_loss: 0.3807
Epoch 20/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3544 - val_loss: 0.3420
Epoch 21/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3427 - val_loss: 0.3452
Epoch 22/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3599 - val_loss: 0.3273
Epoch 23/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3386 - val_loss: 0.3279
Epoch 24/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3312 - val_loss: 0.4346
Epoch 25/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3306 - val_loss: 0.3432
Epoch 26/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3298 - val_loss: 0.3227
Epoch 27/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3436 - val_loss: 0.4466
Epoch 28/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3404 - val_loss: 0.3323
Epoch 29/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3290 - val_loss: 0.3991
Epoch 30/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3298 - val_loss: 0.3434
Epoch 31/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3188 - val_loss: 0.3349
Epoch 32/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3393 - val_loss: 0.3639
Epoch 33/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3286 - val_loss: 0.3461
Epoch 34/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3119 - val_loss: 0.3591
Epoch 35/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3142 - val_loss: 0.3140
Epoch 36/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3092 - val_loss: 0.3636
Epoch 37/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3214 - val_loss: 0.3380
Epoch 38/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3190 - val_loss: 0.5245
Epoch 39/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3297 - val_loss: 0.3326
Epoch 40/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3191 - val_loss: 0.4047
Epoch 41/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3022 - val_loss: 0.3341
Epoch 42/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3030 - val_loss: 0.3658
Epoch 43/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.3322
Epoch 44/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3245 - val_loss: 0.3683
Epoch 45/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3104 - val_loss: 0.3088
Epoch 46/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3077 - val_loss: 0.3814
Epoch 47/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3088 - val_loss: 0.3186
Epoch 48/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3053 - val_loss: 0.3008
Epoch 49/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3110 - val_loss: 0.4070
Epoch 50/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3103 - val_loss: 0.3025
Epoch 51/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3178 - val_loss: 0.3945
Epoch 52/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3020 - val_loss: 0.3052
Epoch 53/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3061 - val_loss: 0.3039
Epoch 54/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3020 - val_loss: 0.2963
Epoch 55/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2999 - val_loss: 0.2948
Epoch 56/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3069 - val_loss: 0.3446
Epoch 57/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3002 - val_loss: 0.3004
Epoch 58/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3080 - val_loss: 0.2959
Epoch 59/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3088 - val_loss: 0.3250
Epoch 60/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3077 - val_loss: 0.3697
Epoch 61/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3138 - val_loss: 0.5512
Epoch 62/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3046 - val_loss: 0.3448
Epoch 63/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3010 - val_loss: 0.4009
Epoch 64/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3153 - val_loss: 0.3734
Epoch 65/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2988 - val_loss: 0.3429
121/121 [==============================] - 0s 822us/step - loss: 0.3114
[CV] END learning_rate=0.0045455096956331, n_hidden=3, n_neurons=30; total time=  32.0s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 3.4800 - val_loss: 29.5063
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 0.8491 - val_loss: 33.7784
Epoch 3/100
242/242 [==============================] - 0s 1ms/step - loss: 0.8416 - val_loss: 4.0125
Epoch 4/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6237 - val_loss: 0.5556
Epoch 5/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5755 - val_loss: 0.5119
Epoch 6/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5306 - val_loss: 0.4888
Epoch 7/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5198 - val_loss: 0.4729
Epoch 8/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4973 - val_loss: 0.4559
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4747 - val_loss: 0.4601
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4664 - val_loss: 0.4303
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4450 - val_loss: 0.4205
Epoch 12/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4195 - val_loss: 0.4242
Epoch 13/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4480 - val_loss: 0.4107
Epoch 14/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4328 - val_loss: 0.4231
Epoch 15/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4293 - val_loss: 0.4221
Epoch 16/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4155 - val_loss: 0.4084
Epoch 17/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4334 - val_loss: 0.4209
Epoch 18/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4284 - val_loss: 0.4017
Epoch 19/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4067 - val_loss: 0.4322
Epoch 20/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.4001
Epoch 21/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4041 - val_loss: 0.4263
Epoch 22/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3951 - val_loss: 0.4032
Epoch 23/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4104 - val_loss: 0.4039
Epoch 24/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3942 - val_loss: 0.3764
Epoch 25/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4024 - val_loss: 0.4241
Epoch 26/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3863 - val_loss: 0.3779
Epoch 27/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4067 - val_loss: 0.4126
Epoch 28/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3956 - val_loss: 0.3967
Epoch 29/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3976 - val_loss: 0.4045
Epoch 30/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.3748
Epoch 31/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3863 - val_loss: 0.3717
Epoch 32/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3825 - val_loss: 0.3676
Epoch 33/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3779 - val_loss: 0.4054
Epoch 34/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 0.3924
Epoch 35/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3782 - val_loss: 0.3611
Epoch 36/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3750 - val_loss: 0.4182
Epoch 37/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3539
Epoch 38/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3741 - val_loss: 0.4403
Epoch 39/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3729 - val_loss: 0.3551
Epoch 40/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.4125
Epoch 41/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3686 - val_loss: 0.3665
Epoch 42/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3748 - val_loss: 0.3591
Epoch 43/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3709 - val_loss: 0.3570
Epoch 44/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.4125
Epoch 45/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3743 - val_loss: 0.3547
Epoch 46/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3585 - val_loss: 0.3779
Epoch 47/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3692 - val_loss: 0.3886
121/121 [==============================] - 0s 740us/step - loss: 0.3877
[CV] END learning_rate=0.0020587676114196545, n_hidden=1, n_neurons=49; total time=  17.8s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 3.3075 - val_loss: 0.7805
Epoch 2/100
242/242 [==============================] - 0s 1ms/step - loss: 0.7479 - val_loss: 1.1550
Epoch 3/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6249 - val_loss: 1.8115
Epoch 4/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5885 - val_loss: 2.6113
Epoch 5/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5412 - val_loss: 3.2626
Epoch 6/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5083 - val_loss: 3.5247
Epoch 7/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4786 - val_loss: 3.5926
Epoch 8/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4660 - val_loss: 3.5562
Epoch 9/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4559 - val_loss: 2.9541
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4427 - val_loss: 2.5606
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4424 - val_loss: 2.1560
121/121 [==============================] - 0s 706us/step - loss: 0.4866
[CV] END learning_rate=0.0020587676114196545, n_hidden=1, n_neurons=49; total time=   4.4s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 2.9276 - val_loss: 2.5834
Epoch 2/100
242/242 [==============================] - 0s 1ms/step - loss: 0.7344 - val_loss: 3.5564
Epoch 3/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6461 - val_loss: 1.7895
Epoch 4/100
242/242 [==============================] - 0s 1ms/step - loss: 0.6260 - val_loss: 1.7436
Epoch 5/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5519 - val_loss: 0.6344
Epoch 6/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5085 - val_loss: 0.8713
Epoch 7/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5055 - val_loss: 0.5604
Epoch 8/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5012 - val_loss: 0.4695
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4772 - val_loss: 0.4942
Epoch 10/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4582 - val_loss: 0.4375
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4637 - val_loss: 0.4536
Epoch 12/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4395 - val_loss: 0.4276
Epoch 13/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4150 - val_loss: 0.4084
Epoch 14/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4219 - val_loss: 0.4897
Epoch 15/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4455 - val_loss: 0.4018
Epoch 16/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4279 - val_loss: 0.5505
Epoch 17/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4079 - val_loss: 0.4602
Epoch 18/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4250 - val_loss: 0.4347
Epoch 19/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4231 - val_loss: 0.3835
Epoch 20/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4083 - val_loss: 0.4115
Epoch 21/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4025 - val_loss: 0.3817
Epoch 22/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4214 - val_loss: 0.3737
Epoch 23/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4011 - val_loss: 0.3720
Epoch 24/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3929 - val_loss: 0.4318
Epoch 25/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3957 - val_loss: 0.4158
Epoch 26/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3918 - val_loss: 0.3821
Epoch 27/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4040 - val_loss: 0.4069
Epoch 28/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3972 - val_loss: 0.4024
Epoch 29/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3859 - val_loss: 0.5904
Epoch 30/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3872 - val_loss: 0.4027
Epoch 31/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.4216
Epoch 32/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4065 - val_loss: 0.3604
Epoch 33/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3831 - val_loss: 0.4134
Epoch 34/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.3633
Epoch 35/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3720 - val_loss: 0.3542
Epoch 36/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3710 - val_loss: 0.3568
Epoch 37/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3822 - val_loss: 0.4216
Epoch 38/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3764 - val_loss: 0.5522
Epoch 39/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3867 - val_loss: 0.5648
Epoch 40/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3818 - val_loss: 0.6416
Epoch 41/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3684 - val_loss: 0.3847
Epoch 42/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.5255
Epoch 43/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3771 - val_loss: 0.7023
Epoch 44/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3837 - val_loss: 0.7508
Epoch 45/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3698 - val_loss: 0.5608
121/121 [==============================] - 0s 1ms/step - loss: 0.3745
[CV] END learning_rate=0.0020587676114196545, n_hidden=1, n_neurons=49; total time=  16.7s
Epoch 1/100
242/242 [==============================] - 1s 3ms/step - loss: 1.6933 - val_loss: 6.4183
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 0.6124 - val_loss: 16.7917
Epoch 3/100
242/242 [==============================] - 1s 4ms/step - loss: 0.5548 - val_loss: 4.7823
Epoch 4/100
242/242 [==============================] - 1s 5ms/step - loss: 0.4575 - val_loss: 8.6077
Epoch 5/100
242/242 [==============================] - 1s 5ms/step - loss: 0.4166 - val_loss: 1.8033
Epoch 6/100
242/242 [==============================] - 1s 5ms/step - loss: 0.4075 - val_loss: 0.3655
Epoch 7/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3664 - val_loss: 0.3784
Epoch 8/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3529 - val_loss: 0.4055
Epoch 9/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3455 - val_loss: 0.3909
Epoch 10/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3489 - val_loss: 0.3910
Epoch 11/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3307 - val_loss: 0.3555
Epoch 12/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3169 - val_loss: 0.3611
Epoch 13/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3465 - val_loss: 0.3653
Epoch 14/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3317 - val_loss: 0.3632
Epoch 15/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3252 - val_loss: 0.3562
Epoch 16/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3238 - val_loss: 0.3559
Epoch 17/100
242/242 [==============================] - 1s 5ms/step - loss: 0.3329 - val_loss: 0.3556
Epoch 18/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3388 - val_loss: 0.3494
Epoch 19/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3133 - val_loss: 0.3533
Epoch 20/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3203 - val_loss: 0.3413
Epoch 21/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3155 - val_loss: 0.3364
Epoch 22/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3059 - val_loss: 0.3530
Epoch 23/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3150 - val_loss: 0.3387
Epoch 24/100
242/242 [==============================] - 1s 5ms/step - loss: 0.3084 - val_loss: 0.3076
Epoch 25/100
242/242 [==============================] - 1s 5ms/step - loss: 0.3103 - val_loss: 0.3483
Epoch 26/100
242/242 [==============================] - 1s 4ms/step - loss: 0.2950 - val_loss: 0.3063
Epoch 27/100
242/242 [==============================] - 1s 4ms/step - loss: 0.3112 - val_loss: 0.3288
Epoch 28/100
242/242 [==============================] - 1s 5ms/step - loss: 0.3055 - val_loss: 0.3452
Epoch 29/100
242/242 [==============================] - 1s 5ms/step - loss: 0.2992 - val_loss: 0.3321
Epoch 30/100
242/242 [==============================] - 1s 5ms/step - loss: 0.2987 - val_loss: 0.2967
Epoch 31/100
242/242 [==============================] - 1s 4ms/step - loss: 0.2952 - val_loss: 0.3079
Epoch 32/100
242/242 [==============================] - 1s 4ms/step - loss: 0.2848 - val_loss: 0.2935
Epoch 33/100
242/242 [==============================] - 1s 3ms/step - loss: 0.2895 - val_loss: 0.3203
Epoch 34/100
242/242 [==============================] - 1s 2ms/step - loss: 0.2830 - val_loss: 0.3131
Epoch 35/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2851 - val_loss: 0.2900
Epoch 36/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2861 - val_loss: 0.3643
Epoch 37/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2759 - val_loss: 0.3348
Epoch 38/100
242/242 [==============================] - 1s 3ms/step - loss: 0.2940 - val_loss: 0.3545
Epoch 39/100
242/242 [==============================] - 1s 3ms/step - loss: 0.2785 - val_loss: 0.2958
Epoch 40/100
242/242 [==============================] - 1s 3ms/step - loss: 0.2661 - val_loss: 0.3321
Epoch 41/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2756 - val_loss: 0.2870
Epoch 42/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2770 - val_loss: 0.2950
Epoch 43/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2786 - val_loss: 0.2850
Epoch 44/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2744 - val_loss: 0.3206
Epoch 45/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2796 - val_loss: 0.3254
Epoch 46/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2667 - val_loss: 0.3312
Epoch 47/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2683 - val_loss: 0.2868
Epoch 48/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2659 - val_loss: 0.2856
Epoch 49/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2788 - val_loss: 0.2862
Epoch 50/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2644 - val_loss: 0.3474
Epoch 51/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2591 - val_loss: 0.3195
Epoch 52/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.3195
Epoch 53/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2640 - val_loss: 0.2824
Epoch 54/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2581 - val_loss: 0.2857
Epoch 55/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2576 - val_loss: 0.2941
Epoch 56/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2756 - val_loss: 0.2839
Epoch 57/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2705 - val_loss: 0.3084
Epoch 58/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2632 - val_loss: 0.2756
Epoch 59/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2684 - val_loss: 0.2859
Epoch 60/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2562 - val_loss: 0.2778
Epoch 61/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2545 - val_loss: 0.3473
Epoch 62/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2582 - val_loss: 0.2803
Epoch 63/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2624 - val_loss: 0.3849
Epoch 64/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2533 - val_loss: 0.2723
Epoch 65/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2456 - val_loss: 0.3335
Epoch 66/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2667 - val_loss: 0.4009
Epoch 67/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2632 - val_loss: 0.4573
Epoch 68/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2571 - val_loss: 0.2717
Epoch 69/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2533 - val_loss: 0.3129
Epoch 70/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2674 - val_loss: 0.2774
Epoch 71/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2568 - val_loss: 0.3059
Epoch 72/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2664 - val_loss: 0.2838
Epoch 73/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2606 - val_loss: 0.2842
Epoch 74/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2620 - val_loss: 0.3037
Epoch 75/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2507 - val_loss: 0.2853
Epoch 76/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2595 - val_loss: 0.2895
Epoch 77/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2486 - val_loss: 0.2767
Epoch 78/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2429 - val_loss: 0.2956
121/121 [==============================] - 0s 2ms/step - loss: 0.3085
[CV] END learning_rate=0.005803602934201024, n_hidden=3, n_neurons=74; total time=  51.3s
Epoch 1/100
242/242 [==============================] - 1s 3ms/step - loss: 1.4417 - val_loss: 0.7369
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5298 - val_loss: 0.4431
Epoch 3/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4286 - val_loss: 0.3919
Epoch 4/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4017 - val_loss: 0.3834
Epoch 5/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3837 - val_loss: 0.3951
Epoch 6/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.4650
Epoch 7/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3497 - val_loss: 0.6408
Epoch 8/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3530 - val_loss: 0.7273
Epoch 9/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3417 - val_loss: 0.9104
Epoch 10/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.6969
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3395 - val_loss: 0.6999
Epoch 12/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3255 - val_loss: 0.7835
Epoch 13/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3186 - val_loss: 0.8539
Epoch 14/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3123 - val_loss: 0.8282
121/121 [==============================] - 0s 770us/step - loss: 0.3525
[CV] END learning_rate=0.005803602934201024, n_hidden=3, n_neurons=74; total time=   6.1s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 1.6880 - val_loss: 0.9196
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4943 - val_loss: 2.1025
Epoch 3/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4287 - val_loss: 3.5511
Epoch 4/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4621 - val_loss: 1.5867
Epoch 5/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3738 - val_loss: 0.4227
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3522 - val_loss: 0.3738
Epoch 7/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3500 - val_loss: 0.3350
Epoch 8/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3672 - val_loss: 0.3384
Epoch 9/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3477 - val_loss: 0.3720
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3507 - val_loss: 0.3276
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3650 - val_loss: 0.3971
Epoch 12/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3353 - val_loss: 0.3329
Epoch 13/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3092 - val_loss: 0.3226
Epoch 14/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3206 - val_loss: 0.3672
Epoch 15/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3210
Epoch 16/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3253 - val_loss: 0.3579
Epoch 17/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3125 - val_loss: 0.3219
Epoch 18/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3227 - val_loss: 0.3558
Epoch 19/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 0.3361
Epoch 20/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3249 - val_loss: 0.3591
Epoch 21/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3151 - val_loss: 0.3142
Epoch 22/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3264 - val_loss: 0.3186
Epoch 23/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3109 - val_loss: 0.3591
Epoch 24/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3026 - val_loss: 0.3072
Epoch 25/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3002 - val_loss: 0.3338
Epoch 26/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2997 - val_loss: 0.3272
Epoch 27/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3140 - val_loss: 0.3118
Epoch 28/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3101 - val_loss: 0.3419
Epoch 29/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3102 - val_loss: 0.3919
Epoch 30/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3024 - val_loss: 0.3055
Epoch 31/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2896 - val_loss: 0.3150
Epoch 32/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3080 - val_loss: 0.3035
Epoch 33/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2980 - val_loss: 0.3303
Epoch 34/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2852 - val_loss: 0.3032
Epoch 35/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2834 - val_loss: 0.2941
Epoch 36/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2783 - val_loss: 0.4024
Epoch 37/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2903 - val_loss: 0.3373
Epoch 38/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2943 - val_loss: 0.4171
Epoch 39/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3001 - val_loss: 0.3120
Epoch 40/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2846 - val_loss: 0.3722
Epoch 41/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2699 - val_loss: 0.3132
Epoch 42/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2793 - val_loss: 0.3368
Epoch 43/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2922 - val_loss: 0.2825
Epoch 44/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2919 - val_loss: 0.3396
Epoch 45/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2745 - val_loss: 0.2980
Epoch 46/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2761 - val_loss: 0.3311
Epoch 47/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2773 - val_loss: 0.2825
Epoch 48/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2725 - val_loss: 0.2851
Epoch 49/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2799 - val_loss: 0.3564
Epoch 50/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2801 - val_loss: 0.2971
Epoch 51/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2823 - val_loss: 0.3396
Epoch 52/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2686 - val_loss: 0.2958
Epoch 53/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2783 - val_loss: 0.2951
121/121 [==============================] - 0s 784us/step - loss: 0.3002
[CV] END learning_rate=0.005803602934201024, n_hidden=3, n_neurons=74; total time=  20.0s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 1.5786 - val_loss: 10.9251
Epoch 2/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5353 - val_loss: 3.3912
Epoch 3/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4521 - val_loss: 0.4039
Epoch 4/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4084 - val_loss: 0.3692
Epoch 5/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3670 - val_loss: 0.3555
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3524 - val_loss: 0.3875
Epoch 7/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3633
Epoch 8/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.3991
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3333 - val_loss: 0.3797
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3379 - val_loss: 0.3703
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3209 - val_loss: 0.3310
Epoch 12/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3109 - val_loss: 0.3511
Epoch 13/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3374 - val_loss: 0.3793
Epoch 14/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3223 - val_loss: 0.3316
Epoch 15/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3175 - val_loss: 0.3488
Epoch 16/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3173 - val_loss: 0.3472
Epoch 17/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3261 - val_loss: 0.3264
Epoch 18/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3305 - val_loss: 0.3422
Epoch 19/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3066 - val_loss: 0.3366
Epoch 20/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3130 - val_loss: 0.3223
Epoch 21/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3089 - val_loss: 0.4002
Epoch 22/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3033 - val_loss: 0.3110
Epoch 23/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3065 - val_loss: 0.3629
Epoch 24/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3039 - val_loss: 0.3637
Epoch 25/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3100 - val_loss: 0.3314
Epoch 26/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2907 - val_loss: 0.3077
Epoch 27/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3087 - val_loss: 0.3396
Epoch 28/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3001 - val_loss: 0.3379
Epoch 29/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2935 - val_loss: 0.3446
Epoch 30/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2946 - val_loss: 0.3101
Epoch 31/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2939 - val_loss: 0.3391
Epoch 32/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2822 - val_loss: 0.3048
Epoch 33/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2867 - val_loss: 0.3312
Epoch 34/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2813 - val_loss: 0.3129
Epoch 35/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2803 - val_loss: 0.2918
Epoch 36/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2825 - val_loss: 0.3515
Epoch 37/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2732 - val_loss: 0.3330
Epoch 38/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2927 - val_loss: 0.3513
Epoch 39/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2794 - val_loss: 0.3001
Epoch 40/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2640 - val_loss: 0.3296
Epoch 41/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2753 - val_loss: 0.2921
Epoch 42/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2750 - val_loss: 0.3098
Epoch 43/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2771 - val_loss: 0.2918
Epoch 44/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2707 - val_loss: 0.3196
Epoch 45/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2755 - val_loss: 0.2798
Epoch 46/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2630 - val_loss: 0.3235
Epoch 47/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2673 - val_loss: 0.3136
Epoch 48/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2627 - val_loss: 0.3066
Epoch 49/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2790 - val_loss: 0.2784
Epoch 50/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2654 - val_loss: 0.3552
Epoch 51/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2557 - val_loss: 0.3143
Epoch 52/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2739 - val_loss: 0.3259
Epoch 53/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2598 - val_loss: 0.3045
Epoch 54/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2540 - val_loss: 0.2954
Epoch 55/100
242/242 [==============================] - 1s 2ms/step - loss: 0.2574 - val_loss: 0.3185
Epoch 56/100
242/242 [==============================] - 1s 2ms/step - loss: 0.2768 - val_loss: 0.2765
Epoch 57/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2679 - val_loss: 0.3349
Epoch 58/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2640 - val_loss: 0.3149
Epoch 59/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2686 - val_loss: 0.2738
Epoch 60/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2557 - val_loss: 0.3173
Epoch 61/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2515 - val_loss: 0.3197
Epoch 62/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2552 - val_loss: 0.3052
Epoch 63/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2630 - val_loss: 0.3356
Epoch 64/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2531 - val_loss: 0.3531
Epoch 65/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2465 - val_loss: 0.2817
Epoch 66/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2685 - val_loss: 0.3063
Epoch 67/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2575 - val_loss: 0.3056
Epoch 68/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2574 - val_loss: 0.3599
Epoch 69/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2521 - val_loss: 0.2705
Epoch 70/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2658 - val_loss: 0.3327
Epoch 71/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2569 - val_loss: 0.2791
Epoch 72/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2671 - val_loss: 0.4311
Epoch 73/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2598 - val_loss: 0.3029
Epoch 74/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2640 - val_loss: 0.4839
Epoch 75/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2524 - val_loss: 0.2760
Epoch 76/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2584 - val_loss: 0.3965
Epoch 77/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2481 - val_loss: 0.3056
Epoch 78/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2464 - val_loss: 0.3750
Epoch 79/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2515 - val_loss: 0.2797
121/121 [==============================] - 0s 784us/step - loss: 0.3016
[CV] END learning_rate=0.0059640580092043885, n_hidden=3, n_neurons=80; total time=  30.3s
Epoch 1/100
242/242 [==============================] - 1s 3ms/step - loss: 1.5488 - val_loss: 0.6551
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4745 - val_loss: 0.4129
Epoch 3/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4052 - val_loss: 0.6096
Epoch 4/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3959 - val_loss: 0.6534
Epoch 5/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3826 - val_loss: 0.6227
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3631 - val_loss: 0.8399
Epoch 7/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3493 - val_loss: 1.0573
Epoch 8/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3563 - val_loss: 1.1232
Epoch 9/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3392 - val_loss: 1.2405
Epoch 10/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.8071
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3414 - val_loss: 0.8317
Epoch 12/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3264 - val_loss: 0.7693
121/121 [==============================] - 0s 763us/step - loss: 0.3595
[CV] END learning_rate=0.0059640580092043885, n_hidden=3, n_neurons=80; total time=   5.4s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 1.4295 - val_loss: 2.2007
Epoch 2/100
242/242 [==============================] - 0s 2ms/step - loss: 0.5148 - val_loss: 3.3028
Epoch 3/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4423 - val_loss: 0.9130
Epoch 4/100
242/242 [==============================] - 0s 2ms/step - loss: 0.4027 - val_loss: 0.5328
Epoch 5/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3757 - val_loss: 0.3609
Epoch 6/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3528 - val_loss: 0.4151
Epoch 7/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3518 - val_loss: 0.3580
Epoch 8/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3723 - val_loss: 0.3516
Epoch 9/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3488 - val_loss: 0.3983
Epoch 10/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3449 - val_loss: 0.3323
Epoch 11/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3567 - val_loss: 0.4231
Epoch 12/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3353 - val_loss: 0.3283
Epoch 13/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3110 - val_loss: 0.3466
Epoch 14/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3199 - val_loss: 0.4040
Epoch 15/100
242/242 [==============================] - 1s 3ms/step - loss: 0.3410 - val_loss: 0.3272
Epoch 16/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3264 - val_loss: 0.3769
Epoch 17/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3130 - val_loss: 0.3209
Epoch 18/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3198 - val_loss: 0.3301
Epoch 19/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3349 - val_loss: 0.3752
Epoch 20/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3248 - val_loss: 0.3440
Epoch 21/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3156 - val_loss: 0.3378
Epoch 22/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3259 - val_loss: 0.3046
Epoch 23/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3079 - val_loss: 0.3115
Epoch 24/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3026 - val_loss: 0.4024
Epoch 25/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2999 - val_loss: 0.3425
Epoch 26/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3016 - val_loss: 0.3284
Epoch 27/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3122 - val_loss: 0.3661
Epoch 28/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3144 - val_loss: 0.4084
Epoch 29/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3092 - val_loss: 0.3410
Epoch 30/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3022 - val_loss: 0.3331
Epoch 31/100
242/242 [==============================] - 0s 2ms/step - loss: 0.2912 - val_loss: 0.3120
Epoch 32/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3093 - val_loss: 0.3405
121/121 [==============================] - 0s 804us/step - loss: 0.3036
[CV] END learning_rate=0.0059640580092043885, n_hidden=3, n_neurons=80; total time=  13.4s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 2.1527 - val_loss: 0.5753
Epoch 2/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5713 - val_loss: 8.9879
Epoch 3/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5292 - val_loss: 11.0986
Epoch 4/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5126 - val_loss: 1.1306
Epoch 5/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4298 - val_loss: 0.5258
Epoch 6/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3984 - val_loss: 0.4499
Epoch 7/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3937 - val_loss: 0.4056
Epoch 8/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3816 - val_loss: 0.3998
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3687 - val_loss: 0.3957
Epoch 10/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3701 - val_loss: 0.3903
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3687
Epoch 12/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3396 - val_loss: 0.3650
Epoch 13/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3672 - val_loss: 0.3709
Epoch 14/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.3816
Epoch 15/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3518 - val_loss: 0.3620
Epoch 16/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3670
Epoch 17/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.3669
Epoch 18/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3619 - val_loss: 0.3606
Epoch 19/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3404 - val_loss: 0.3551
Epoch 20/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3443 - val_loss: 0.3535
Epoch 21/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3381 - val_loss: 0.3521
Epoch 22/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3313 - val_loss: 0.3474
Epoch 23/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3413 - val_loss: 0.3531
Epoch 24/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3382 - val_loss: 0.3298
Epoch 25/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3398 - val_loss: 0.3679
Epoch 26/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3262 - val_loss: 0.3244
Epoch 27/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3448 - val_loss: 0.3421
Epoch 28/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3376 - val_loss: 0.3408
Epoch 29/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3318 - val_loss: 0.3430
Epoch 30/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3302 - val_loss: 0.3208
Epoch 31/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3299 - val_loss: 0.3226
Epoch 32/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3205 - val_loss: 0.3150
Epoch 33/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3229 - val_loss: 0.3509
Epoch 34/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3197 - val_loss: 0.3158
Epoch 35/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3166 - val_loss: 0.3141
Epoch 36/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3178 - val_loss: 0.3836
Epoch 37/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3101 - val_loss: 0.3596
Epoch 38/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3258 - val_loss: 0.3542
Epoch 39/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3116 - val_loss: 0.3175
Epoch 40/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3007 - val_loss: 0.3486
Epoch 41/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3138 - val_loss: 0.3187
Epoch 42/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3160 - val_loss: 0.3190
Epoch 43/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.3086
Epoch 44/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3092 - val_loss: 0.3505
Epoch 45/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3166 - val_loss: 0.3051
Epoch 46/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3001 - val_loss: 0.3303
Epoch 47/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3051 - val_loss: 0.3350
Epoch 48/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2949 - val_loss: 0.3231
Epoch 49/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3179 - val_loss: 0.3016
Epoch 50/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3041 - val_loss: 0.3612
Epoch 51/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2900 - val_loss: 0.3298
Epoch 52/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3164 - val_loss: 0.3131
Epoch 53/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2994 - val_loss: 0.3143
Epoch 54/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2968 - val_loss: 0.3070
Epoch 55/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2971 - val_loss: 0.3698
Epoch 56/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3188 - val_loss: 0.2995
Epoch 57/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3057 - val_loss: 0.3884
Epoch 58/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3020 - val_loss: 0.3142
Epoch 59/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3065 - val_loss: 0.3033
Epoch 60/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3000 - val_loss: 0.3136
Epoch 61/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2892 - val_loss: 0.3676
Epoch 62/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2932 - val_loss: 0.3380
Epoch 63/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3018 - val_loss: 0.3356
Epoch 64/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2897 - val_loss: 0.3342
Epoch 65/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2802 - val_loss: 0.3091
Epoch 66/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3091 - val_loss: 0.2988
Epoch 67/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2983 - val_loss: 0.3918
Epoch 68/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2933 - val_loss: 0.3026
Epoch 69/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2925 - val_loss: 0.3563
Epoch 70/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3142 - val_loss: 0.3104
Epoch 71/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2976 - val_loss: 0.4927
Epoch 72/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3106 - val_loss: 0.3005
Epoch 73/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2984 - val_loss: 0.2985
Epoch 74/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2992 - val_loss: 0.3542
Epoch 75/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2868 - val_loss: 0.3016
Epoch 76/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2943 - val_loss: 0.5721
Epoch 77/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2832 - val_loss: 0.8889
Epoch 78/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2971 - val_loss: 0.8371
Epoch 79/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3073 - val_loss: 0.3383
Epoch 80/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2850 - val_loss: 0.3761
Epoch 81/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2747 - val_loss: 0.2930
Epoch 82/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2751 - val_loss: 0.3313
Epoch 83/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2687 - val_loss: 0.2939
Epoch 84/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2910 - val_loss: 0.3487
Epoch 85/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2880 - val_loss: 0.2920
Epoch 86/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2715 - val_loss: 0.2984
Epoch 87/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2721 - val_loss: 0.3051
Epoch 88/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2867 - val_loss: 0.3356
Epoch 89/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2813 - val_loss: 0.2880
Epoch 90/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2695 - val_loss: 0.4234
Epoch 91/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2941 - val_loss: 0.3515
Epoch 92/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2944 - val_loss: 0.4546
Epoch 93/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3065 - val_loss: 0.4674
Epoch 94/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3153 - val_loss: 0.4872
Epoch 95/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2658 - val_loss: 0.3859
Epoch 96/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2761 - val_loss: 0.7000
Epoch 97/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2821 - val_loss: 0.8526
Epoch 98/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2835 - val_loss: 0.8769
Epoch 99/100
242/242 [==============================] - 0s 1ms/step - loss: 0.2767 - val_loss: 0.3565
121/121 [==============================] - 0s 745us/step - loss: 0.3209
[CV] END learning_rate=0.004591455636549438, n_hidden=2, n_neurons=59; total time=  34.5s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 2.2993 - val_loss: 0.8898
Epoch 2/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5586 - val_loss: 0.5270
Epoch 3/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4526 - val_loss: 0.4844
Epoch 4/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4289 - val_loss: 0.4250
Epoch 5/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4117 - val_loss: 0.3735
Epoch 6/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3892 - val_loss: 0.3859
Epoch 7/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3723 - val_loss: 0.4576
Epoch 8/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.4928
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3636 - val_loss: 0.6246
Epoch 10/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3692 - val_loss: 0.5255
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3671 - val_loss: 0.5956
Epoch 12/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3472 - val_loss: 0.6364
Epoch 13/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3456 - val_loss: 0.7456
Epoch 14/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3352 - val_loss: 0.7136
Epoch 15/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3650 - val_loss: 0.6905
121/121 [==============================] - 0s 993us/step - loss: 0.3615
[CV] END learning_rate=0.004591455636549438, n_hidden=2, n_neurons=59; total time=   5.9s
Epoch 1/100
242/242 [==============================] - 1s 2ms/step - loss: 2.1229 - val_loss: 2.8528
Epoch 2/100
242/242 [==============================] - 1s 2ms/step - loss: 0.6059 - val_loss: 2.3412
Epoch 3/100
242/242 [==============================] - 0s 1ms/step - loss: 0.5163 - val_loss: 0.9015
Epoch 4/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4576 - val_loss: 0.8313
Epoch 5/100
242/242 [==============================] - 0s 1ms/step - loss: 0.4213 - val_loss: 0.5217
Epoch 6/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3916 - val_loss: 0.4956
Epoch 7/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3878 - val_loss: 0.3745
Epoch 8/100
242/242 [==============================] - 1s 3ms/step - loss: 0.4044 - val_loss: 0.4012
Epoch 9/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3805 - val_loss: 0.4169
Epoch 10/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3843
Epoch 11/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3900 - val_loss: 0.6122
Epoch 12/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.3579
Epoch 13/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3399 - val_loss: 0.3497
Epoch 14/100
242/242 [==============================] - 1s 2ms/step - loss: 0.3488 - val_loss: 0.5161
Epoch 15/100
242/242 [==============================] - 0s 2ms/step - loss: 0.3736 - val_loss: 0.4273
Epoch 16/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3580 - val_loss: 0.5739
Epoch 17/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.4975
Epoch 18/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3557 - val_loss: 0.4887
Epoch 19/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.3371
Epoch 20/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.4123
Epoch 21/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3310
Epoch 22/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3579 - val_loss: 0.3289
Epoch 23/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3393 - val_loss: 0.3287
Epoch 24/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.5231
Epoch 25/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.7723
Epoch 26/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3354 - val_loss: 0.8988
Epoch 27/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3821 - val_loss: 0.4934
Epoch 28/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3446 - val_loss: 0.6236
Epoch 29/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.3448
Epoch 30/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 0.5678
Epoch 31/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3230 - val_loss: 0.3712
Epoch 32/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.7379
Epoch 33/100
242/242 [==============================] - 0s 1ms/step - loss: 0.3364 - val_loss: 0.3972
121/121 [==============================] - 0s 765us/step - loss: 0.3361
[CV] END learning_rate=0.004591455636549438, n_hidden=2, n_neurons=59; total time=  12.8s
Epoch 1/100
363/363 [==============================] - 1s 2ms/step - loss: 1.4030 - val_loss: 1.8036
Epoch 2/100
363/363 [==============================] - 0s 1ms/step - loss: 0.5715 - val_loss: 2.0827
Epoch 3/100
363/363 [==============================] - 0s 1ms/step - loss: 0.4548 - val_loss: 0.3796
Epoch 4/100
363/363 [==============================] - 0s 1ms/step - loss: 0.4007 - val_loss: 0.4283
Epoch 5/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3832 - val_loss: 0.3617
Epoch 6/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.4566
Epoch 7/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.3573
Epoch 8/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3580 - val_loss: 0.3380
Epoch 9/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3757
Epoch 10/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3433 - val_loss: 0.4069
Epoch 11/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3312 - val_loss: 0.5455
Epoch 12/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3456 - val_loss: 0.6470
Epoch 13/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.3109
Epoch 14/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3259 - val_loss: 0.3198
Epoch 15/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3222 - val_loss: 0.3065
Epoch 16/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3277 - val_loss: 0.3252
Epoch 17/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3095 - val_loss: 0.3965
Epoch 18/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3107 - val_loss: 0.2997
Epoch 19/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3060 - val_loss: 0.3079
Epoch 20/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3003 - val_loss: 0.4544
Epoch 21/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3090 - val_loss: 0.3274
Epoch 22/100
363/363 [==============================] - 0s 1ms/step - loss: 0.2949 - val_loss: 0.5018
Epoch 23/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3126 - val_loss: 0.5565
Epoch 24/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3031 - val_loss: 0.5390
Epoch 25/100
363/363 [==============================] - 0s 1ms/step - loss: 0.2992 - val_loss: 0.3339
Epoch 26/100
363/363 [==============================] - 0s 1ms/step - loss: 0.2988 - val_loss: 0.5095
Epoch 27/100
363/363 [==============================] - 0s 1ms/step - loss: 0.3001 - val_loss: 0.6597
Epoch 28/100
363/363 [==============================] - 1s 1ms/step - loss: 0.3058 - val_loss: 0.5106
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RandomizedSearchCV(cv=3,
                   estimator=&lt;tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x143110370&gt;,
                   param_distributions={&#39;learning_rate&#39;: [0.001683454924600351,
                                                          0.02390836445593178,
                                                          0.008731907739399206,
                                                          0.004725396149933917,
                                                          0.0006154014789262348,
                                                          0.0006153331256530192,
                                                          0.0003920021771415983,
                                                          0.01619845322936229,
                                                          0.004779156784872302,
                                                          0.0078...
                                                          0.005021425736625637,
                                                          0.0005703073595961105,
                                                          0.001151888789941251,
                                                          0.001621231156394198,
                                                          0.0024505367684280487,
                                                          0.011155092541719619,
                                                          0.0007524347058135697,
                                                          0.0032032448128444043,
                                                          0.004591455636549438,
                                                          0.0003715541189658278, ...],
                                        &#39;n_hidden&#39;: [0, 1, 2, 3],
                                        &#39;n_neurons&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9,
                                                      10, 11, 12, 13, 14, 15,
                                                      16, 17, 18, 19, 20, 21,
                                                      22, 23, 24, 25, 26, 27,
                                                      28, 29, 30, ...]},
                   verbose=2)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rnd_search_cv</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;n_neurons&#39;: 74, &#39;n_hidden&#39;: 3, &#39;learning_rate&#39;: 0.005803602934201024}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rnd_search_cv</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-0.32039451599121094
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rnd_search_cv</span><span class="o">.</span><span class="n">best_estimator_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor at 0x140555f70&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rnd_search_cv</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>162/162 [==============================] - 0s 2ms/step - loss: 0.3029
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-0.3028871417045593
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">rnd_search_cv</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">model</span>
<span class="n">model</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow.python.keras.engine.sequential.Sequential at 0x13fbcf340&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>162/162 [==============================] - 0s 2ms/step - loss: 0.3029
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.3028871417045593
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="exercise-solutions">
<h1>Exercise solutions<a class="headerlink" href="#exercise-solutions" title="Permalink to this headline">¶</a></h1>
<p><em>Exercise: Train a deep MLP on the MNIST dataset (you can load it using <code class="docutils literal notranslate"><span class="pre">keras.datasets.mnist.load_data()</span></code>. See if you can get over 98% precision. Try searching for the optimal learning rate by using the approach presented in this chapter (i.e., by growing the learning rate exponentially, plotting the loss, and finding the point where the loss shoots up). Try adding all the bells and whistles—save checkpoints, use early stopping, and plot learning curves using TensorBoard.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">X_train_full</span><span class="p">,</span> <span class="n">y_train_full</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Just like for the Fashion MNIST dataset, the MNIST training set contains 60,000 grayscale images, each 28x28 pixels:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_full</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(60000, 28, 28)
</pre></div>
</div>
</div>
</div>
<p>Each pixel intensity is also represented as a byte (0 to 255):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_full</span><span class="o">.</span><span class="n">dtype</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dtype(&#39;uint8&#39;)
</pre></div>
</div>
</div>
</div>
<p>Let’s split the full training set into a validation set and a (smaller) training set. We also scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255, just like we did for Fashion MNIST:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_valid</span><span class="p">,</span> <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train_full</span><span class="p">[:</span><span class="mi">5000</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.</span><span class="p">,</span> <span class="n">X_train_full</span><span class="p">[</span><span class="mi">5000</span><span class="p">:]</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="n">y_valid</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train_full</span><span class="p">[:</span><span class="mi">5000</span><span class="p">],</span> <span class="n">y_train_full</span><span class="p">[</span><span class="mi">5000</span><span class="p">:]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span> <span class="o">/</span> <span class="mf">255.</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s plot an image using Matplotlib’s <code class="docutils literal notranslate"><span class="pre">imshow()</span></code> function, with a <code class="docutils literal notranslate"><span class="pre">'binary'</span></code>
color map:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/101-intro-keras_132_0.png" src="../_images/101-intro-keras_132_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([7, 3, 4, ..., 5, 6, 8], dtype=uint8)
</pre></div>
</div>
</div>
</div>
<p>The validation set contains 5,000 images, and the test set contains 10,000 images:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_valid</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(5000, 28, 28)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(10000, 28, 28)
</pre></div>
</div>
</div>
</div>
<p>Let’s take a look at a sample of the images in the dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span>
                       <span class="mi">10</span><span class="p">,</span>
                       <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span>
                       <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                       <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">():</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">y_train</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;small&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/101-intro-keras_138_0.png" src="../_images/101-intro-keras_138_0.png" />
</div>
</div>
<p>Let’s build a simple dense network and find the optimal learning rate. We will need a callback to grow the learning rate at each iteration. It will also record the learning rate and the loss at each iteration:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">backend</span>

<span class="k">class</span> <span class="nc">ExponentialLearningRate</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">factor</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rates</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">def</span> <span class="nf">on_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">logs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">lr</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">])</span>
        <span class="n">K</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We will start with a small learning rate of 1e-3, and grow it by 0.5% at each iteration:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;sparse_categorical_crossentropy&quot;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
              
<span class="n">expon_lr</span> <span class="o">=</span> <span class="n">ExponentialLearningRate</span><span class="p">(</span><span class="n">factor</span><span class="o">=</span><span class="mf">1.005</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s train the model for just 1 epoch:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">expon_lr</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1719/1719 [==============================] - 2s 1ms/step - loss: 4.6604 - accuracy: 0.4887 - val_loss: 2.3911 - val_accuracy: 0.1126
</pre></div>
</div>
</div>
</div>
<p>We can now plot the loss as a functionof the learning rate:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">expon_lr</span><span class="o">.</span><span class="n">rates</span><span class="p">,</span> <span class="n">expon_lr</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">expon_lr</span><span class="o">.</span><span class="n">losses</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="n">expon_lr</span><span class="o">.</span><span class="n">rates</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">expon_lr</span><span class="o">.</span><span class="n">rates</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="nb">min</span><span class="p">(</span><span class="n">expon_lr</span><span class="o">.</span><span class="n">rates</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">expon_lr</span><span class="o">.</span><span class="n">rates</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">expon_lr</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Learning rate&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Loss&#39;)
</pre></div>
</div>
<img alt="../_images/101-intro-keras_148_1.png" src="../_images/101-intro-keras_148_1.png" />
</div>
</div>
<p>The loss starts shooting back up violently when the learning rate goes over 6e-1, so let’s try using half of that, at 3e-1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;sparse_categorical_crossentropy&quot;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">3e-1</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">run_index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">run_logdir</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;data/101/y_mnist_logs/run_</span><span class="si">{</span><span class="n">run_index</span><span class="si">:</span><span class="s2">03d</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">run_logdir</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;./my_mnist_logs/run_001&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">early_stopping_cb</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">checkpoint_cb</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s2">&quot;data/101/my_mnist_model3.h5&quot;</span><span class="p">,</span> <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tensorboard_cb</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">run_logdir</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint_cb</span><span class="p">,</span> <span class="n">early_stopping_cb</span><span class="p">,</span> <span class="n">tensorboard_cb</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/100
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4195 - accuracy: 0.8677 - val_loss: 0.0995 - val_accuracy: 0.9724
Epoch 2/100
1719/1719 [==============================] - 2s 882us/step - loss: 0.0941 - accuracy: 0.9698 - val_loss: 0.0913 - val_accuracy: 0.9746
Epoch 3/100
1719/1719 [==============================] - 1s 845us/step - loss: 0.0650 - accuracy: 0.9792 - val_loss: 0.0785 - val_accuracy: 0.9772
Epoch 4/100
1719/1719 [==============================] - 2s 932us/step - loss: 0.0438 - accuracy: 0.9855 - val_loss: 0.0793 - val_accuracy: 0.9784
Epoch 5/100
1719/1719 [==============================] - 1s 832us/step - loss: 0.0348 - accuracy: 0.9888 - val_loss: 0.0724 - val_accuracy: 0.9812
Epoch 6/100
1719/1719 [==============================] - 1s 835us/step - loss: 0.0289 - accuracy: 0.9905 - val_loss: 0.0814 - val_accuracy: 0.9792
Epoch 7/100
1719/1719 [==============================] - 1s 868us/step - loss: 0.0230 - accuracy: 0.9926 - val_loss: 0.0794 - val_accuracy: 0.9808
Epoch 8/100
1719/1719 [==============================] - 1s 847us/step - loss: 0.0180 - accuracy: 0.9943 - val_loss: 0.0718 - val_accuracy: 0.9826
Epoch 9/100
1719/1719 [==============================] - 1s 848us/step - loss: 0.0158 - accuracy: 0.9949 - val_loss: 0.0874 - val_accuracy: 0.9798
Epoch 10/100
1719/1719 [==============================] - 1s 844us/step - loss: 0.0155 - accuracy: 0.9944 - val_loss: 0.0782 - val_accuracy: 0.9824
Epoch 11/100
1719/1719 [==============================] - 1s 834us/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.0902 - val_accuracy: 0.9832
Epoch 12/100
1719/1719 [==============================] - 1s 844us/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0832 - val_accuracy: 0.9832
Epoch 13/100
1719/1719 [==============================] - 1s 859us/step - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.0888 - val_accuracy: 0.9814
Epoch 14/100
1719/1719 [==============================] - 2s 919us/step - loss: 0.0110 - accuracy: 0.9963 - val_loss: 0.1080 - val_accuracy: 0.9792
Epoch 15/100
1719/1719 [==============================] - 2s 921us/step - loss: 0.0075 - accuracy: 0.9973 - val_loss: 0.0828 - val_accuracy: 0.9840
Epoch 16/100
1719/1719 [==============================] - 2s 945us/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0869 - val_accuracy: 0.9848
Epoch 17/100
1719/1719 [==============================] - 2s 962us/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.0997 - val_accuracy: 0.9816
Epoch 18/100
1719/1719 [==============================] - 2s 976us/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.1001 - val_accuracy: 0.9840
Epoch 19/100
1719/1719 [==============================] - 2s 1ms/step - loss: 0.0086 - accuracy: 0.9972 - val_loss: 0.1239 - val_accuracy: 0.9796
Epoch 20/100
1719/1719 [==============================] - 2s 1ms/step - loss: 0.0095 - accuracy: 0.9973 - val_loss: 0.1107 - val_accuracy: 0.9808
Epoch 21/100
1719/1719 [==============================] - 2s 1ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.0891 - val_accuracy: 0.9840
Epoch 22/100
1719/1719 [==============================] - 2s 967us/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0893 - val_accuracy: 0.9844
Epoch 23/100
1719/1719 [==============================] - 2s 963us/step - loss: 6.1009e-04 - accuracy: 0.9999 - val_loss: 0.0899 - val_accuracy: 0.9848
Epoch 24/100
1719/1719 [==============================] - 2s 972us/step - loss: 8.4212e-05 - accuracy: 1.0000 - val_loss: 0.0894 - val_accuracy: 0.9862
Epoch 25/100
1719/1719 [==============================] - 2s 1ms/step - loss: 6.0306e-05 - accuracy: 1.0000 - val_loss: 0.0899 - val_accuracy: 0.9858
Epoch 26/100
1719/1719 [==============================] - 2s 1ms/step - loss: 4.9564e-05 - accuracy: 1.0000 - val_loss: 0.0903 - val_accuracy: 0.9860
Epoch 27/100
1719/1719 [==============================] - 2s 1ms/step - loss: 4.3609e-05 - accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 0.9862
Epoch 28/100
1719/1719 [==============================] - 2s 973us/step - loss: 4.2216e-05 - accuracy: 1.0000 - val_loss: 0.0911 - val_accuracy: 0.9862
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;data/101/my_mnist_model3.h5&quot;</span><span class="p">)</span> <span class="c1"># rollback to best model</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>313/313 [==============================] - 0s 701us/step - loss: 0.0804 - accuracy: 0.9806
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.08043695986270905, 0.9805999994277954]
</pre></div>
</div>
</div>
</div>
<p>We got over 98% accuracy. Finally, let’s look at the learning curves using TensorBoard:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">tensorboard</span> --logdir=./my_mnist_logs --port=6006
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<iframe id="tensorboard-frame-27a48e88c728a23e" width="100%" height="800" frameborder="0">
</iframe>
<script>
  (function() {
    const frame = document.getElementById("tensorboard-frame-27a48e88c728a23e");
    const url = new URL("/", window.location);
    const port = 6006;
    if (port) {
      url.port = port;
    }
    frame.src = url;
  })();
</script>
</div></div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./handson-dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="../handson/07-ensemble-learning.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Ensemble Learning</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="102-training-deep-neural-networks.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Training Deep Neural Networks</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By integzz<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>